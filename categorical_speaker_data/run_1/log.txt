{'output_directory': 'save_folder', 'grid_size': 4, 'min_other_objects': 4, 'max_objects': 4, 'min_object_size': 1, 'max_object_size': 4, 'other_objects_sample_percentage': 0.3, 'obstacles_flag': False, 'num_obstacles': 5, 'enable_maze': False, 'maze_complexity': 0, 'maze_density': 0, 'type_grammar': 'simple_intrans', 'intransitive_verbs': 'walk', 'transitive_verbs': 'push,pull,pickup,drop', 'nouns': 'circle,square,cylinder,diamond', 'color_adjectives': 'red,blue,yellow,green', 'size_adjectives': '', 'keep_fixed_weights': True, 'all_light': True, 'num_episodes': 300000, 'episode_len': 10, 'grid_input_type': 'vector', 'comm_type': 'categorical', 'comm_setting': 'cheap_talk', 'temp': 1, 'lights_out': False, 'render_episode': False, 'wait_time': 0.3}
Episode: 1 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8050, grad_fn=<SubBackward0>)
Episode: 2 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 3 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 4 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 5 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 6 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 7 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 8 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 9 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8528, grad_fn=<SubBackward0>)
Episode: 10 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 11 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 12 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 13 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 14 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 15 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 16 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 17 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2129, grad_fn=<SubBackward0>)
Episode: 18 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 19 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 20 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 21 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 22 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0325, grad_fn=<SubBackward0>)
Episode: 23 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 24 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3054, grad_fn=<SubBackward0>)
Episode: 25 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 26 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1612, grad_fn=<SubBackward0>)
Episode: 27 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 28 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2895, grad_fn=<SubBackward0>)
Episode: 29 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 30 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 31 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8737, grad_fn=<SubBackward0>)
Episode: 32 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1355, grad_fn=<SubBackward0>)
Episode: 33 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 34 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 35 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2369, grad_fn=<SubBackward0>)
Episode: 36 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 37 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 38 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 39 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 40 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 41 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 42 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 43 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 44 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 45 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 46 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 47 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 48 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 49 | Train Reward: tensor([1.]) | Train Loss: tensor(1.4014, grad_fn=<SubBackward0>)
Episode: 50 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 51 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 52 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 53 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 54 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 55 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 56 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 57 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 58 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0364, grad_fn=<SubBackward0>)
Episode: 59 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 60 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9986, grad_fn=<SubBackward0>)
Episode: 61 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 62 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 63 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 64 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 65 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3304, grad_fn=<SubBackward0>)
Episode: 66 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8473, grad_fn=<SubBackward0>)
Episode: 67 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 68 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1872, grad_fn=<SubBackward0>)
Episode: 69 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 70 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 71 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 72 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0500, grad_fn=<SubBackward0>)
Episode: 73 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 74 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 75 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 76 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 77 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 78 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 79 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 80 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1470, grad_fn=<SubBackward0>)
Episode: 81 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 82 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 83 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 84 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0643, grad_fn=<SubBackward0>)
Episode: 85 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7522, grad_fn=<SubBackward0>)
Episode: 86 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 87 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 88 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 89 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 90 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
