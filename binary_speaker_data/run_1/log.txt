{'output_directory': 'save_folder', 'grid_size': 4, 'min_other_objects': 4, 'max_objects': 4, 'min_object_size': 1, 'max_object_size': 4, 'other_objects_sample_percentage': 0.3, 'obstacles_flag': False, 'num_obstacles': 5, 'enable_maze': False, 'maze_complexity': 0, 'maze_density': 0, 'type_grammar': 'simple_intrans', 'intransitive_verbs': 'walk', 'transitive_verbs': 'push,pull,pickup,drop', 'nouns': 'circle,square,cylinder,diamond', 'color_adjectives': 'red,blue,yellow,green', 'size_adjectives': '', 'keep_fixed_weights': True, 'all_light': True, 'num_episodes': 300000, 'episode_len': 10, 'grid_input_type': 'vector', 'comm_type': 'binary', 'comm_setting': 'cheap_talk', 'temp': 1, 'lights_out': False, 'render_episode': False, 'wait_time': 0.3}
Episode: 1 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9850, grad_fn=<SubBackward0>)
Episode: 2 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3191, grad_fn=<SubBackward0>)
Episode: 3 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 4 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0483, grad_fn=<SubBackward0>)
Episode: 5 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7698, grad_fn=<SubBackward0>)
Episode: 6 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9369, grad_fn=<SubBackward0>)
Episode: 7 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 8 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 9 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0180, grad_fn=<SubBackward0>)
Episode: 10 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 11 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0106, grad_fn=<SubBackward0>)
Episode: 12 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 13 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 14 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 15 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 16 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 17 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 18 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 19 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 20 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1693, grad_fn=<SubBackward0>)
Episode: 21 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 22 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 23 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 24 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 25 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 26 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 27 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1386, grad_fn=<SubBackward0>)
Episode: 28 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 29 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 30 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 31 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 32 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 33 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2635, grad_fn=<SubBackward0>)
Episode: 34 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 35 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 36 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 37 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 38 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 39 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 40 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 41 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 42 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 43 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 44 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 45 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 46 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 47 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2703, grad_fn=<SubBackward0>)
Episode: 48 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 49 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1385, grad_fn=<SubBackward0>)
Episode: 50 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8320, grad_fn=<SubBackward0>)
Episode: 51 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 52 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 53 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7500, grad_fn=<SubBackward0>)
Episode: 54 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1515, grad_fn=<SubBackward0>)
Episode: 55 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 56 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 57 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 58 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 59 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9754, grad_fn=<SubBackward0>)
Episode: 60 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3877, grad_fn=<SubBackward0>)
Episode: 61 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 62 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 63 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0653, grad_fn=<SubBackward0>)
Episode: 64 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 65 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7635, grad_fn=<SubBackward0>)
Episode: 66 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 67 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 68 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 69 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 70 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 71 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 72 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 73 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 74 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 75 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 76 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0678, grad_fn=<SubBackward0>)
Episode: 77 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 78 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 79 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 80 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 81 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1843, grad_fn=<SubBackward0>)
Episode: 82 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 83 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 84 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 85 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 86 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 87 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8917, grad_fn=<SubBackward0>)
Episode: 88 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8470, grad_fn=<SubBackward0>)
Episode: 89 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2939, grad_fn=<SubBackward0>)
Episode: 90 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 91 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2150, grad_fn=<SubBackward0>)
Episode: 92 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9860, grad_fn=<SubBackward0>)
Episode: 93 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 94 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 95 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 96 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 97 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 98 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 99 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 100 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 101 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 102 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 103 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 104 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3469, grad_fn=<SubBackward0>)
Episode: 105 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 106 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 107 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 108 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 109 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 110 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 111 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 112 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 113 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 114 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 115 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2732, grad_fn=<SubBackward0>)
Episode: 116 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 117 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 118 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 119 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2373, grad_fn=<SubBackward0>)
Episode: 120 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 121 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 122 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 123 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2379, grad_fn=<SubBackward0>)
Episode: 124 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 125 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 126 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 127 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 128 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1839, grad_fn=<SubBackward0>)
Episode: 129 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7192, grad_fn=<SubBackward0>)
Episode: 130 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 131 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8280, grad_fn=<SubBackward0>)
Episode: 132 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 133 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 134 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 135 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 136 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 137 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9620, grad_fn=<SubBackward0>)
Episode: 138 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 139 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 140 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0710, grad_fn=<SubBackward0>)
Episode: 141 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0428, grad_fn=<SubBackward0>)
Episode: 142 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 143 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1688, grad_fn=<SubBackward0>)
Episode: 144 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 145 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9310, grad_fn=<SubBackward0>)
Episode: 146 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 147 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 148 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 149 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 150 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2288, grad_fn=<SubBackward0>)
Episode: 151 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 152 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 153 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 154 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9105, grad_fn=<SubBackward0>)
Episode: 155 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1632, grad_fn=<SubBackward0>)
Episode: 156 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 157 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0840, grad_fn=<SubBackward0>)
Episode: 158 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9800, grad_fn=<SubBackward0>)
Episode: 159 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 160 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 161 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 162 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9337, grad_fn=<SubBackward0>)
Episode: 163 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 164 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 165 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 166 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 167 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 168 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 169 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 170 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 171 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 172 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 173 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 174 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 175 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1064, grad_fn=<SubBackward0>)
Episode: 176 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 177 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 178 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 179 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 180 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2166, grad_fn=<SubBackward0>)
Episode: 181 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 182 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 183 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 184 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0509, grad_fn=<SubBackward0>)
Episode: 185 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6936, grad_fn=<SubBackward0>)
Episode: 186 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9819, grad_fn=<SubBackward0>)
Episode: 187 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 188 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0240, grad_fn=<SubBackward0>)
Episode: 189 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8300, grad_fn=<SubBackward0>)
Episode: 190 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 191 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 192 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 193 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2338, grad_fn=<SubBackward0>)
Episode: 194 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 195 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 196 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7364, grad_fn=<SubBackward0>)
Episode: 197 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 198 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 199 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 200 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 201 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 202 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 203 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 204 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8857, grad_fn=<SubBackward0>)
Episode: 205 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 206 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 207 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1436, grad_fn=<SubBackward0>)
Episode: 208 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 209 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9567, grad_fn=<SubBackward0>)
Episode: 210 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 211 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9527, grad_fn=<SubBackward0>)
Episode: 212 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 213 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 214 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 215 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 216 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1642, grad_fn=<SubBackward0>)
Episode: 217 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 218 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 219 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 220 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 221 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 222 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 223 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9584, grad_fn=<SubBackward0>)
Episode: 224 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 225 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1136, grad_fn=<SubBackward0>)
Episode: 226 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 227 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 228 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.8364, grad_fn=<SubBackward0>)
Episode: 229 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 230 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 231 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 232 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 233 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 234 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 235 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9723, grad_fn=<SubBackward0>)
Episode: 236 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 237 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 238 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 239 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 240 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 241 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 242 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 243 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 244 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 245 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 246 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 247 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 248 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 249 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 250 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6451, grad_fn=<SubBackward0>)
Episode: 251 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 252 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 253 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 254 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 255 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 256 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 257 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 258 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 259 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 260 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 261 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 262 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 263 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 264 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 265 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 266 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 267 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 268 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 269 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 270 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 271 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 272 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7312, grad_fn=<SubBackward0>)
Episode: 273 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1472, grad_fn=<SubBackward0>)
Episode: 274 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9258, grad_fn=<SubBackward0>)
Episode: 275 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 276 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2203, grad_fn=<SubBackward0>)
Episode: 277 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9607, grad_fn=<SubBackward0>)
Episode: 278 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 279 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 280 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 281 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3697, grad_fn=<SubBackward0>)
Episode: 282 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 283 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 284 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 285 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 286 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 287 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7600, grad_fn=<SubBackward0>)
Episode: 288 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 289 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 290 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 291 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 292 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 293 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 294 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 295 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9828, grad_fn=<SubBackward0>)
Episode: 296 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 297 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 298 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 299 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 300 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 301 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0022, grad_fn=<SubBackward0>)
Episode: 302 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 303 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 304 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9746, grad_fn=<SubBackward0>)
Episode: 305 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 306 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0861, grad_fn=<SubBackward0>)
Episode: 307 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 308 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2647, grad_fn=<SubBackward0>)
Episode: 309 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 310 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 311 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 312 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 313 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9825, grad_fn=<SubBackward0>)
Episode: 314 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8483, grad_fn=<SubBackward0>)
Episode: 315 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 316 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 317 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 318 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8045, grad_fn=<SubBackward0>)
Episode: 319 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 320 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 321 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 322 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 323 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 324 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 325 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 326 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 327 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 328 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9440, grad_fn=<SubBackward0>)
Episode: 329 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 330 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 331 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 332 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 333 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 334 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7706, grad_fn=<SubBackward0>)
Episode: 335 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 336 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 337 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 338 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 339 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 340 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 341 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 342 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 343 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9581, grad_fn=<SubBackward0>)
Episode: 344 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 345 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 346 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 347 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 348 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 349 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 350 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9775, grad_fn=<SubBackward0>)
Episode: 351 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 352 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 353 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 354 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 355 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 356 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 357 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 358 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 359 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 360 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 361 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 362 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9315, grad_fn=<SubBackward0>)
Episode: 363 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 364 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9092, grad_fn=<SubBackward0>)
Episode: 365 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 366 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 367 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 368 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9661, grad_fn=<SubBackward0>)
Episode: 369 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 370 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 371 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 372 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 373 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9083, grad_fn=<SubBackward0>)
Episode: 374 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8568, grad_fn=<SubBackward0>)
Episode: 375 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0003, grad_fn=<SubBackward0>)
Episode: 376 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 377 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 378 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 379 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 380 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6667, grad_fn=<SubBackward0>)
Episode: 381 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 382 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 383 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 384 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8860, grad_fn=<SubBackward0>)
Episode: 385 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 386 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 387 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 388 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 389 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 390 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 391 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 392 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 393 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 394 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 395 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 396 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7087, grad_fn=<SubBackward0>)
Episode: 397 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 398 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8857, grad_fn=<SubBackward0>)
Episode: 399 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 400 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 401 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 402 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7032, grad_fn=<SubBackward0>)
Episode: 403 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 404 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 405 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 406 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 407 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.2080, grad_fn=<SubBackward0>)
Episode: 408 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 409 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 410 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 411 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 412 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8156, grad_fn=<SubBackward0>)
Episode: 413 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 414 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 415 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 416 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 417 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 418 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 419 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 420 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 421 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 422 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1669, grad_fn=<SubBackward0>)
Episode: 423 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 424 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 425 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 426 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 427 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 428 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 429 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9246, grad_fn=<SubBackward0>)
Episode: 430 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 431 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 432 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 433 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 434 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 435 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 436 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 437 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6675, grad_fn=<SubBackward0>)
Episode: 438 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 439 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 440 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3572, grad_fn=<SubBackward0>)
Episode: 441 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2576, grad_fn=<SubBackward0>)
Episode: 442 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 443 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 444 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 445 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 446 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9497, grad_fn=<SubBackward0>)
Episode: 447 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 448 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 449 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7206, grad_fn=<SubBackward0>)
Episode: 450 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 451 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 452 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 453 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 454 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 455 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0533, grad_fn=<SubBackward0>)
Episode: 456 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 457 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 458 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 459 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 460 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 461 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0662, grad_fn=<SubBackward0>)
Episode: 462 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 463 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8423, grad_fn=<SubBackward0>)
Episode: 464 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 465 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 466 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 467 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 468 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 469 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8950, grad_fn=<SubBackward0>)
Episode: 470 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 471 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8183, grad_fn=<SubBackward0>)
Episode: 472 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 473 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 474 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 475 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 476 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 477 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 478 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8127, grad_fn=<SubBackward0>)
Episode: 479 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8690, grad_fn=<SubBackward0>)
Episode: 480 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 481 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 482 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.7752, grad_fn=<SubBackward0>)
Episode: 483 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 484 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8641, grad_fn=<SubBackward0>)
Episode: 485 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 486 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 487 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 488 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 489 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9165, grad_fn=<SubBackward0>)
Episode: 490 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1243, grad_fn=<SubBackward0>)
Episode: 491 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1243, grad_fn=<SubBackward0>)
Episode: 492 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 493 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 494 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8748, grad_fn=<SubBackward0>)
Episode: 495 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 496 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 497 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1230, grad_fn=<SubBackward0>)
Episode: 498 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 499 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 500 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 501 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1221, grad_fn=<SubBackward0>)
Episode: 502 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7371, grad_fn=<SubBackward0>)
Episode: 503 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1233, grad_fn=<SubBackward0>)
Episode: 504 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 505 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 506 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2037, grad_fn=<SubBackward0>)
Episode: 507 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 508 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 509 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7111, grad_fn=<SubBackward0>)
Episode: 510 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1236, grad_fn=<SubBackward0>)
Episode: 511 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 512 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6056, grad_fn=<SubBackward0>)
Episode: 513 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 514 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 515 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8572, grad_fn=<SubBackward0>)
Episode: 516 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 517 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 518 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 519 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 520 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1238, grad_fn=<SubBackward0>)
Episode: 521 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 522 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 523 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1222, grad_fn=<SubBackward0>)
Episode: 524 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 525 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 526 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 527 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 528 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 529 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 530 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 531 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 532 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 533 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 534 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 535 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 536 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 537 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 538 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 539 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 540 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 541 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 542 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 543 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 544 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 545 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 546 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 547 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3360, grad_fn=<SubBackward0>)
Episode: 548 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 549 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 550 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 551 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 552 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6860, grad_fn=<SubBackward0>)
Episode: 553 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 554 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 555 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 556 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0793, grad_fn=<SubBackward0>)
Episode: 557 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 558 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 559 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 560 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0069, grad_fn=<SubBackward0>)
Episode: 561 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9660, grad_fn=<SubBackward0>)
Episode: 562 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9557, grad_fn=<SubBackward0>)
Episode: 563 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 564 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 565 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3876, grad_fn=<SubBackward0>)
Episode: 566 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 567 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 568 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 569 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 570 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 571 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9886, grad_fn=<SubBackward0>)
Episode: 572 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 573 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8373, grad_fn=<SubBackward0>)
Episode: 574 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 575 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 576 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 577 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7620, grad_fn=<SubBackward0>)
Episode: 578 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 579 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 580 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 581 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 582 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0418, grad_fn=<SubBackward0>)
Episode: 583 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 584 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 585 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 586 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 587 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 588 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 589 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 590 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 591 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 592 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 593 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 594 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0226, grad_fn=<SubBackward0>)
Episode: 595 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 596 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 597 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 598 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 599 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 600 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 601 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 602 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 603 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 604 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9583, grad_fn=<SubBackward0>)
Episode: 605 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 606 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 607 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 608 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 609 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8282, grad_fn=<SubBackward0>)
Episode: 610 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8911, grad_fn=<SubBackward0>)
Episode: 611 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 612 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 613 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2581, grad_fn=<SubBackward0>)
Episode: 614 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 615 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 616 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 617 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 618 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 619 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 620 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 621 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 622 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 623 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 624 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.7405, grad_fn=<SubBackward0>)
Episode: 625 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 626 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 627 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1366, grad_fn=<SubBackward0>)
Episode: 628 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 629 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 630 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 631 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 632 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 633 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 634 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 635 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 636 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 637 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7768, grad_fn=<SubBackward0>)
Episode: 638 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 639 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 640 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 641 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 642 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 643 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.5232, grad_fn=<SubBackward0>)
Episode: 644 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3816, grad_fn=<SubBackward0>)
Episode: 645 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 646 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 647 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 648 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 649 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 650 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 651 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 652 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 653 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9187, grad_fn=<SubBackward0>)
Episode: 654 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 655 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 656 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7606, grad_fn=<SubBackward0>)
Episode: 657 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 658 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 659 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9413, grad_fn=<SubBackward0>)
Episode: 660 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 661 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 662 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 663 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 664 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 665 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7468, grad_fn=<SubBackward0>)
Episode: 666 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 667 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.2951, grad_fn=<SubBackward0>)
Episode: 668 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 669 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9489, grad_fn=<SubBackward0>)
Episode: 670 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 671 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 672 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 673 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 674 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 675 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 676 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 677 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 678 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 679 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 680 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 681 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 682 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 683 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 684 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 685 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0112, grad_fn=<SubBackward0>)
Episode: 686 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2327, grad_fn=<SubBackward0>)
Episode: 687 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 688 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 689 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 690 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 691 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 692 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 693 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9506, grad_fn=<SubBackward0>)
Episode: 694 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9205, grad_fn=<SubBackward0>)
Episode: 695 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8046, grad_fn=<SubBackward0>)
Episode: 696 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 697 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 698 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 699 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3784, grad_fn=<SubBackward0>)
Episode: 700 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 701 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 702 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 703 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 704 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 705 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7406, grad_fn=<SubBackward0>)
Episode: 706 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0599, grad_fn=<SubBackward0>)
Episode: 707 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 708 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 709 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 710 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 711 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 712 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 713 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 714 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 715 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 716 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 717 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 718 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 719 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 720 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 721 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 722 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 723 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.3168, grad_fn=<SubBackward0>)
Episode: 724 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2516, grad_fn=<SubBackward0>)
Episode: 725 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8888, grad_fn=<SubBackward0>)
Episode: 726 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 727 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 728 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 729 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9454, grad_fn=<SubBackward0>)
Episode: 730 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 731 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 732 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 733 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 734 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.8997, grad_fn=<SubBackward0>)
Episode: 735 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 736 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 737 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 738 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9513, grad_fn=<SubBackward0>)
Episode: 739 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1871, grad_fn=<SubBackward0>)
Episode: 740 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 741 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 742 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 743 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 744 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 745 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 746 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9684, grad_fn=<SubBackward0>)
Episode: 747 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 748 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 749 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 750 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 751 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 752 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 753 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 754 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 755 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 756 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 757 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8588, grad_fn=<SubBackward0>)
Episode: 758 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 759 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 760 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 761 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 762 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 763 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 764 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 765 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 766 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 767 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 768 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 769 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 770 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 771 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 772 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 773 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9193, grad_fn=<SubBackward0>)
Episode: 774 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 775 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 776 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 777 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 778 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9270, grad_fn=<SubBackward0>)
Episode: 779 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 780 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 781 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7290, grad_fn=<SubBackward0>)
Episode: 782 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 783 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3769, grad_fn=<SubBackward0>)
Episode: 784 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 785 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9329, grad_fn=<SubBackward0>)
Episode: 786 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 787 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 788 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8007, grad_fn=<SubBackward0>)
Episode: 789 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 790 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7360, grad_fn=<SubBackward0>)
Episode: 791 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 792 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 793 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9006, grad_fn=<SubBackward0>)
Episode: 794 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 795 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 796 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 797 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1385, grad_fn=<SubBackward0>)
Episode: 798 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8250, grad_fn=<SubBackward0>)
Episode: 799 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 800 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 801 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 802 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 803 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 804 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 805 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 806 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 807 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 808 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6950, grad_fn=<SubBackward0>)
Episode: 809 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 810 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.4328, grad_fn=<SubBackward0>)
Episode: 811 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 812 | Train Reward: tensor([1.]) | Train Loss: tensor(0.7887, grad_fn=<SubBackward0>)
Episode: 813 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 814 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 815 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 816 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 817 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 818 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1243, grad_fn=<SubBackward0>)
Episode: 819 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 820 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 821 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 822 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 823 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7138, grad_fn=<SubBackward0>)
Episode: 824 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 825 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 826 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 827 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 828 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 829 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7649, grad_fn=<SubBackward0>)
Episode: 830 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 831 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 832 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 833 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 834 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 835 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 836 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 837 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 838 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 839 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 840 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 841 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 842 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3835, grad_fn=<SubBackward0>)
Episode: 843 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 844 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1236, grad_fn=<SubBackward0>)
Episode: 845 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 846 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 847 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8923, grad_fn=<SubBackward0>)
Episode: 848 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 849 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 850 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 851 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3029, grad_fn=<SubBackward0>)
Episode: 852 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 853 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 854 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 855 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7529, grad_fn=<SubBackward0>)
Episode: 856 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 857 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 858 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 859 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 860 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 861 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 862 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8095, grad_fn=<SubBackward0>)
Episode: 863 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 864 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 865 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 866 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 867 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 868 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 869 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 870 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 871 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 872 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 873 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 874 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 875 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 876 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 877 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 878 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8748, grad_fn=<SubBackward0>)
Episode: 879 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 880 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 881 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 882 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8487, grad_fn=<SubBackward0>)
Episode: 883 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 884 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 885 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 886 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 887 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8760, grad_fn=<SubBackward0>)
Episode: 888 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 889 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 890 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 891 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1230, grad_fn=<SubBackward0>)
Episode: 892 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9387, grad_fn=<SubBackward0>)
Episode: 893 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 894 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.5998, grad_fn=<SubBackward0>)
Episode: 895 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1231, grad_fn=<SubBackward0>)
Episode: 896 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 897 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6542, grad_fn=<SubBackward0>)
Episode: 898 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 899 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 900 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 901 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1215, grad_fn=<SubBackward0>)
Episode: 902 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6249, grad_fn=<SubBackward0>)
Episode: 903 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 904 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 905 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6587, grad_fn=<SubBackward0>)
Episode: 906 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 907 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1217, grad_fn=<SubBackward0>)
Episode: 908 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1243, grad_fn=<SubBackward0>)
Episode: 909 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1236, grad_fn=<SubBackward0>)
Episode: 910 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 911 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1212, grad_fn=<SubBackward0>)
Episode: 912 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 913 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8482, grad_fn=<SubBackward0>)
Episode: 914 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 915 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1207, grad_fn=<SubBackward0>)
Episode: 916 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1215, grad_fn=<SubBackward0>)
Episode: 917 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1223, grad_fn=<SubBackward0>)
Episode: 918 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 919 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 920 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1670, grad_fn=<SubBackward0>)
Episode: 921 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 922 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 923 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 924 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1216, grad_fn=<SubBackward0>)
Episode: 925 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 926 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 927 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 928 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 929 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 930 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 931 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 932 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.5416, grad_fn=<SubBackward0>)
Episode: 933 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 934 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 935 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 936 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 937 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0178, grad_fn=<SubBackward0>)
Episode: 938 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1340, grad_fn=<SubBackward0>)
Episode: 939 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6811, grad_fn=<SubBackward0>)
Episode: 940 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 941 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 942 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8309, grad_fn=<SubBackward0>)
Episode: 943 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2292, grad_fn=<SubBackward0>)
Episode: 944 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 945 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 946 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 947 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 948 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 949 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 950 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 951 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 952 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 953 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1540, grad_fn=<SubBackward0>)
Episode: 954 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2262, grad_fn=<SubBackward0>)
Episode: 955 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0480, grad_fn=<SubBackward0>)
Episode: 956 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 957 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 958 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 959 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2543, grad_fn=<SubBackward0>)
Episode: 960 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 961 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 962 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 963 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 964 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8225, grad_fn=<SubBackward0>)
Episode: 965 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7918, grad_fn=<SubBackward0>)
Episode: 966 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0741, grad_fn=<SubBackward0>)
Episode: 967 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 968 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 969 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 970 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9772, grad_fn=<SubBackward0>)
Episode: 971 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 972 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8166, grad_fn=<SubBackward0>)
Episode: 973 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 974 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 975 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 976 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 977 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 978 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 979 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 980 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 981 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 982 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 983 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 984 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.2328, grad_fn=<SubBackward0>)
Episode: 985 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9751, grad_fn=<SubBackward0>)
Episode: 986 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 987 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 988 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0823, grad_fn=<SubBackward0>)
Episode: 989 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8091, grad_fn=<SubBackward0>)
Episode: 990 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 991 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 992 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 993 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 994 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 995 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 996 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 997 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 998 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 999 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9627, grad_fn=<SubBackward0>)
Episode: 1000 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1001 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0039, grad_fn=<SubBackward0>)
Episode: 1002 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1003 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1004 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0080, grad_fn=<SubBackward0>)
Episode: 1005 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9758, grad_fn=<SubBackward0>)
Episode: 1006 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1007 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1008 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0005, grad_fn=<SubBackward0>)
Episode: 1009 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1010 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1011 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1012 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1013 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1014 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1015 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1016 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9963, grad_fn=<SubBackward0>)
Episode: 1017 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1018 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1019 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1020 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.7479, grad_fn=<SubBackward0>)
Episode: 1021 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1022 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2528, grad_fn=<SubBackward0>)
Episode: 1023 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1024 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2359, grad_fn=<SubBackward0>)
Episode: 1025 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.7973, grad_fn=<SubBackward0>)
Episode: 1026 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6471, grad_fn=<SubBackward0>)
Episode: 1027 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1028 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1029 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 1030 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 1031 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1236, grad_fn=<SubBackward0>)
Episode: 1032 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1033 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1034 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 1035 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1036 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1037 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 1038 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 1039 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1224, grad_fn=<SubBackward0>)
Episode: 1040 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 1041 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1229, grad_fn=<SubBackward0>)
Episode: 1042 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1465, grad_fn=<SubBackward0>)
Episode: 1043 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 1044 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 1045 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 1046 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 1047 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1048 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1049 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9247, grad_fn=<SubBackward0>)
Episode: 1050 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1051 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6165, grad_fn=<SubBackward0>)
Episode: 1052 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 1053 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1054 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 1055 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 1056 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 1057 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 1058 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 1059 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1060 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1469, grad_fn=<SubBackward0>)
Episode: 1061 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 1062 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1063 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1064 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9625, grad_fn=<SubBackward0>)
Episode: 1065 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1765, grad_fn=<SubBackward0>)
Episode: 1066 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 1067 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 1068 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 1069 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1070 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1071 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1072 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3284, grad_fn=<SubBackward0>)
Episode: 1073 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 1074 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1075 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7684, grad_fn=<SubBackward0>)
Episode: 1076 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9363, grad_fn=<SubBackward0>)
Episode: 1077 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1078 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1079 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1080 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8641, grad_fn=<SubBackward0>)
Episode: 1081 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1082 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1083 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1084 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1085 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1086 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1087 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1088 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1089 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9145, grad_fn=<SubBackward0>)
Episode: 1090 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1091 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 1092 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2039, grad_fn=<SubBackward0>)
Episode: 1093 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0690, grad_fn=<SubBackward0>)
Episode: 1094 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2232, grad_fn=<SubBackward0>)
Episode: 1095 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1096 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2912, grad_fn=<SubBackward0>)
Episode: 1097 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1098 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1099 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1100 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1101 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8351, grad_fn=<SubBackward0>)
Episode: 1102 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8363, grad_fn=<SubBackward0>)
Episode: 1103 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3048, grad_fn=<SubBackward0>)
Episode: 1104 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1105 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1106 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1107 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1108 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1109 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1110 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2322, grad_fn=<SubBackward0>)
Episode: 1111 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1112 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1113 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1114 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9535, grad_fn=<SubBackward0>)
Episode: 1115 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1116 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1117 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1118 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1119 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7421, grad_fn=<SubBackward0>)
Episode: 1120 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0946, grad_fn=<SubBackward0>)
Episode: 1121 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1122 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7347, grad_fn=<SubBackward0>)
Episode: 1123 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1124 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1125 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1126 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1127 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1128 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2520, grad_fn=<SubBackward0>)
Episode: 1129 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1130 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1131 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1132 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1133 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1134 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1135 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1136 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1137 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1138 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1139 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9166, grad_fn=<SubBackward0>)
Episode: 1140 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1141 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9060, grad_fn=<SubBackward0>)
Episode: 1142 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1143 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1144 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 1145 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1146 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1147 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1148 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1149 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1150 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1151 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1152 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1153 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1154 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9667, grad_fn=<SubBackward0>)
Episode: 1155 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1156 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1157 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9995, grad_fn=<SubBackward0>)
Episode: 1158 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1159 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2365, grad_fn=<SubBackward0>)
Episode: 1160 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1161 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1162 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1163 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1164 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1165 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1166 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1167 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1168 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1169 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1170 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1171 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8189, grad_fn=<SubBackward0>)
Episode: 1172 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1173 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1174 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1175 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1176 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1177 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1178 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1179 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1180 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1181 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0193, grad_fn=<SubBackward0>)
Episode: 1182 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1183 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 1184 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1185 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1186 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1187 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1188 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3231, grad_fn=<SubBackward0>)
Episode: 1189 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8777, grad_fn=<SubBackward0>)
Episode: 1190 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1191 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1192 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1193 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1194 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0924, grad_fn=<SubBackward0>)
Episode: 1195 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1196 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1600, grad_fn=<SubBackward0>)
Episode: 1197 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1355, grad_fn=<SubBackward0>)
Episode: 1198 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1199 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1200 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1201 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1202 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1203 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1204 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8912, grad_fn=<SubBackward0>)
Episode: 1205 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1206 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1207 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0443, grad_fn=<SubBackward0>)
Episode: 1208 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1209 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9500, grad_fn=<SubBackward0>)
Episode: 1210 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1211 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1212 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1213 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1214 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1215 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1216 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1217 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2421, grad_fn=<SubBackward0>)
Episode: 1218 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1219 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1220 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1221 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1222 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7709, grad_fn=<SubBackward0>)
Episode: 1223 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1224 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1225 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1226 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9197, grad_fn=<SubBackward0>)
Episode: 1227 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9820, grad_fn=<SubBackward0>)
Episode: 1228 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1229 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8615, grad_fn=<SubBackward0>)
Episode: 1230 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9410, grad_fn=<SubBackward0>)
Episode: 1231 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1232 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1233 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1234 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1235 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 1236 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 1237 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1238 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1239 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 1240 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8111, grad_fn=<SubBackward0>)
Episode: 1241 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1242 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1243 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1244 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1245 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.2711, grad_fn=<SubBackward0>)
Episode: 1246 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1247 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 1248 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1249 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1250 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1251 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1252 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9214, grad_fn=<SubBackward0>)
Episode: 1253 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1503, grad_fn=<SubBackward0>)
Episode: 1254 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8080, grad_fn=<SubBackward0>)
Episode: 1255 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1256 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1257 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1258 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1259 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1260 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.5892, grad_fn=<SubBackward0>)
Episode: 1261 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1262 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1263 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1264 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1265 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1266 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1267 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1268 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1269 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1270 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1271 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1272 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1273 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1274 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1275 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1276 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1277 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1278 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1279 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1280 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1281 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1282 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1283 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1284 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1285 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1286 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1287 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1288 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1289 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9375, grad_fn=<SubBackward0>)
Episode: 1290 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1291 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1780, grad_fn=<SubBackward0>)
Episode: 1292 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0246, grad_fn=<SubBackward0>)
Episode: 1293 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1294 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1295 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1296 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 1297 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1298 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1299 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1300 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1301 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1302 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9009, grad_fn=<SubBackward0>)
Episode: 1303 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1304 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1305 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1306 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1307 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8758, grad_fn=<SubBackward0>)
Episode: 1308 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1309 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1310 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1311 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1312 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1313 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1314 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1315 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1316 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1317 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1318 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1319 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1320 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1321 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1322 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1323 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1324 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1325 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 1326 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1327 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1328 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1329 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1330 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1331 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1332 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1333 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1334 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1335 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8281, grad_fn=<SubBackward0>)
Episode: 1336 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1337 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1338 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1339 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7136, grad_fn=<SubBackward0>)
Episode: 1340 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0897, grad_fn=<SubBackward0>)
Episode: 1341 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1342 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1343 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1344 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1345 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 1346 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1347 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9763, grad_fn=<SubBackward0>)
Episode: 1348 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1349 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1350 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7774, grad_fn=<SubBackward0>)
Episode: 1351 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 1352 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1353 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 1354 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1355 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 1356 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1357 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0254, grad_fn=<SubBackward0>)
Episode: 1358 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1359 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1360 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1361 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 1362 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 1363 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 1364 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1567, grad_fn=<SubBackward0>)
Episode: 1365 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1687, grad_fn=<SubBackward0>)
Episode: 1366 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1367 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8218, grad_fn=<SubBackward0>)
Episode: 1368 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1369 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1007, grad_fn=<SubBackward0>)
Episode: 1370 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0727, grad_fn=<SubBackward0>)
Episode: 1371 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0343, grad_fn=<SubBackward0>)
Episode: 1372 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 1373 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1374 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1375 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1376 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1377 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1378 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1379 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1380 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1381 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0432, grad_fn=<SubBackward0>)
Episode: 1382 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1383 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1384 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1385 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1386 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1387 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1388 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1389 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1390 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 1391 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1392 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1393 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2822, grad_fn=<SubBackward0>)
Episode: 1394 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1395 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1396 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1397 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1398 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1399 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1400 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1401 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1402 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2960, grad_fn=<SubBackward0>)
Episode: 1403 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1404 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1405 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1406 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1407 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1408 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7604, grad_fn=<SubBackward0>)
Episode: 1409 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1410 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0110, grad_fn=<SubBackward0>)
Episode: 1411 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1412 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1413 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1414 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1415 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1416 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1417 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0084, grad_fn=<SubBackward0>)
Episode: 1418 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1419 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1420 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1421 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1422 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1423 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1424 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1425 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1426 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1427 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1428 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1429 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 1430 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1431 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1432 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1433 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0278, grad_fn=<SubBackward0>)
Episode: 1434 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 1435 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 1436 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1437 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1457, grad_fn=<SubBackward0>)
Episode: 1438 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1439 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1440 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1441 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 1442 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9032, grad_fn=<SubBackward0>)
Episode: 1443 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1444 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1445 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1446 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1447 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1448 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1449 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1450 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1451 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1452 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1453 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1454 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1455 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1456 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1457 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1458 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1459 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1460 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2591, grad_fn=<SubBackward0>)
Episode: 1461 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1462 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9382, grad_fn=<SubBackward0>)
Episode: 1463 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8956, grad_fn=<SubBackward0>)
Episode: 1464 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1465 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 1466 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1467 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1468 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1469 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1470 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1471 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1472 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1473 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0563, grad_fn=<SubBackward0>)
Episode: 1474 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 1475 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9727, grad_fn=<SubBackward0>)
Episode: 1476 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1477 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1478 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7921, grad_fn=<SubBackward0>)
Episode: 1479 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1480 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1481 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1482 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8885, grad_fn=<SubBackward0>)
Episode: 1483 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7792, grad_fn=<SubBackward0>)
Episode: 1484 | Train Reward: tensor([1.]) | Train Loss: tensor(1.4011, grad_fn=<SubBackward0>)
Episode: 1485 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1486 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1487 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1488 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1654, grad_fn=<SubBackward0>)
Episode: 1489 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1490 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1491 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8947, grad_fn=<SubBackward0>)
Episode: 1492 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 1493 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0355, grad_fn=<SubBackward0>)
Episode: 1494 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1495 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1496 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1497 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 1498 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8938, grad_fn=<SubBackward0>)
Episode: 1499 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1500 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3235, grad_fn=<SubBackward0>)
Episode: 1501 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1502 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1503 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1504 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8477, grad_fn=<SubBackward0>)
Episode: 1505 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1506 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8915, grad_fn=<SubBackward0>)
Episode: 1507 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1508 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1509 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1135, grad_fn=<SubBackward0>)
Episode: 1510 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0157, grad_fn=<SubBackward0>)
Episode: 1511 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9949, grad_fn=<SubBackward0>)
Episode: 1512 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1513 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9238, grad_fn=<SubBackward0>)
Episode: 1514 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1515 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1516 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0603, grad_fn=<SubBackward0>)
Episode: 1517 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1518 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0194, grad_fn=<SubBackward0>)
Episode: 1519 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1520 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1521 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1522 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1523 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1524 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1525 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1526 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2543, grad_fn=<SubBackward0>)
Episode: 1527 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1528 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1529 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1530 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0122, grad_fn=<SubBackward0>)
Episode: 1531 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1532 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1533 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1534 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1535 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1536 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1537 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1538 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0787, grad_fn=<SubBackward0>)
Episode: 1539 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1540 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0585, grad_fn=<SubBackward0>)
Episode: 1541 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8382, grad_fn=<SubBackward0>)
Episode: 1542 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1543 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9599, grad_fn=<SubBackward0>)
Episode: 1544 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1545 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1546 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1547 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1548 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6569, grad_fn=<SubBackward0>)
Episode: 1549 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1550 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1551 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1552 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1553 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1554 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1555 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8280, grad_fn=<SubBackward0>)
Episode: 1556 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1871, grad_fn=<SubBackward0>)
Episode: 1557 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1558 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1559 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 1560 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9610, grad_fn=<SubBackward0>)
Episode: 1561 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 1562 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1563 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1564 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1565 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1566 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2558, grad_fn=<SubBackward0>)
Episode: 1567 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1568 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1569 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1570 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1571 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0235, grad_fn=<SubBackward0>)
Episode: 1572 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1573 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1574 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1575 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1576 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1577 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1578 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1579 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1580 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1581 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1582 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7775, grad_fn=<SubBackward0>)
Episode: 1583 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1584 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 1585 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1620, grad_fn=<SubBackward0>)
Episode: 1586 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1587 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1588 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1589 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1590 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1591 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1592 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1593 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1594 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 1595 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1596 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7491, grad_fn=<SubBackward0>)
Episode: 1597 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1598 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 1599 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1600 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1601 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0055, grad_fn=<SubBackward0>)
Episode: 1602 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1603 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 1604 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1605 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1606 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8316, grad_fn=<SubBackward0>)
Episode: 1607 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1693, grad_fn=<SubBackward0>)
Episode: 1608 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1790, grad_fn=<SubBackward0>)
Episode: 1609 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 1610 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 1611 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1612 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9415, grad_fn=<SubBackward0>)
Episode: 1613 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 1614 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1615 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 1616 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0515, grad_fn=<SubBackward0>)
Episode: 1617 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1618 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1619 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2798, grad_fn=<SubBackward0>)
Episode: 1620 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1621 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1622 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 1623 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1624 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1625 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 1626 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 1627 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1628 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7193, grad_fn=<SubBackward0>)
Episode: 1629 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1630 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1357, grad_fn=<SubBackward0>)
Episode: 1631 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1632 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1633 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 1634 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1635 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 1636 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8766, grad_fn=<SubBackward0>)
Episode: 1637 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.2675, grad_fn=<SubBackward0>)
Episode: 1638 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1639 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1465, grad_fn=<SubBackward0>)
Episode: 1640 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7812, grad_fn=<SubBackward0>)
Episode: 1641 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0556, grad_fn=<SubBackward0>)
Episode: 1642 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9031, grad_fn=<SubBackward0>)
Episode: 1643 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1644 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1645 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1646 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1647 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0438, grad_fn=<SubBackward0>)
Episode: 1648 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1649 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1650 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1651 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1652 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9330, grad_fn=<SubBackward0>)
Episode: 1653 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1654 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1655 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8861, grad_fn=<SubBackward0>)
Episode: 1656 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1657 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1658 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1523, grad_fn=<SubBackward0>)
Episode: 1659 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1660 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1661 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1662 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1663 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1664 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1665 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1666 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1667 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9962, grad_fn=<SubBackward0>)
Episode: 1668 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 1669 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1670 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1116, grad_fn=<SubBackward0>)
Episode: 1671 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1672 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1673 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1674 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1675 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1676 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1677 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1678 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0426, grad_fn=<SubBackward0>)
Episode: 1679 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1680 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1681 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1682 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1683 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1684 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1685 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0173, grad_fn=<SubBackward0>)
Episode: 1686 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1687 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1688 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1689 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1690 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1691 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1692 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1693 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1694 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1695 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1696 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1697 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1698 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1699 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1700 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1701 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1702 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1703 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1704 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1705 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7459, grad_fn=<SubBackward0>)
Episode: 1706 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1707 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9341, grad_fn=<SubBackward0>)
Episode: 1708 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1709 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1710 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1711 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1712 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1713 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1714 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1715 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1716 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1717 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1718 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1719 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0290, grad_fn=<SubBackward0>)
Episode: 1720 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1721 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1722 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8267, grad_fn=<SubBackward0>)
Episode: 1723 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1724 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1725 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1726 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1727 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1728 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1729 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0443, grad_fn=<SubBackward0>)
Episode: 1730 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1731 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1732 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1733 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 1734 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1735 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1736 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1737 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1738 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1739 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1740 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1741 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7873, grad_fn=<SubBackward0>)
Episode: 1742 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1743 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7169, grad_fn=<SubBackward0>)
Episode: 1744 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1745 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1746 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1747 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1748 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1749 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1750 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1751 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1752 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8344, grad_fn=<SubBackward0>)
Episode: 1753 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1754 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1755 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1756 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1757 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1758 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1759 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8274, grad_fn=<SubBackward0>)
Episode: 1760 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1761 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1762 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1763 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9163, grad_fn=<SubBackward0>)
Episode: 1764 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1765 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1766 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1767 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1768 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1769 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1770 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1771 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1772 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0871, grad_fn=<SubBackward0>)
Episode: 1773 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3747, grad_fn=<SubBackward0>)
Episode: 1774 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1775 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1776 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1777 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1778 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7812, grad_fn=<SubBackward0>)
Episode: 1779 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0770, grad_fn=<SubBackward0>)
Episode: 1780 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0294, grad_fn=<SubBackward0>)
Episode: 1781 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1782 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1783 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1784 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1785 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1786 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1787 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1788 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1789 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1790 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 1791 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1792 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1793 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1794 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7624, grad_fn=<SubBackward0>)
Episode: 1795 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1796 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 1797 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1798 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1799 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9580, grad_fn=<SubBackward0>)
Episode: 1800 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1801 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1802 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1803 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1804 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1805 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.2292, grad_fn=<SubBackward0>)
Episode: 1806 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1807 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1808 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1809 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 1810 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1202, grad_fn=<SubBackward0>)
Episode: 1811 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1812 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3563, grad_fn=<SubBackward0>)
Episode: 1813 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1814 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1140, grad_fn=<SubBackward0>)
Episode: 1815 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1816 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9599, grad_fn=<SubBackward0>)
Episode: 1817 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1818 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1190, grad_fn=<SubBackward0>)
Episode: 1819 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2149, grad_fn=<SubBackward0>)
Episode: 1820 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1821 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1822 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8929, grad_fn=<SubBackward0>)
Episode: 1823 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9441, grad_fn=<SubBackward0>)
Episode: 1824 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1825 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0533, grad_fn=<SubBackward0>)
Episode: 1826 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1827 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1828 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1829 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9602, grad_fn=<SubBackward0>)
Episode: 1830 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 1831 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1462, grad_fn=<SubBackward0>)
Episode: 1832 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1833 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1834 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1835 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 1836 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 1837 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1838 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 1839 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 1840 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 1841 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 1842 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1236, grad_fn=<SubBackward0>)
Episode: 1843 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 1844 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 1845 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 1846 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1847 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1848 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8033, grad_fn=<SubBackward0>)
Episode: 1849 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1850 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0406, grad_fn=<SubBackward0>)
Episode: 1851 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 1852 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1853 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1854 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1855 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.7877, grad_fn=<SubBackward0>)
Episode: 1856 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1857 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1289, grad_fn=<SubBackward0>)
Episode: 1858 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1859 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1860 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1861 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1862 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 1863 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1864 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1865 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 1866 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8722, grad_fn=<SubBackward0>)
Episode: 1867 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1868 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1869 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1870 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 1871 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 1872 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7736, grad_fn=<SubBackward0>)
Episode: 1873 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 1874 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 1875 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1876 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1877 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2350, grad_fn=<SubBackward0>)
Episode: 1878 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 1879 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 1880 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1881 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1882 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1883 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1884 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1885 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1886 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1887 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1888 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1889 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1890 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1891 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9956, grad_fn=<SubBackward0>)
Episode: 1892 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8454, grad_fn=<SubBackward0>)
Episode: 1893 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 1894 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1895 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3788, grad_fn=<SubBackward0>)
Episode: 1896 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1897 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1898 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3159, grad_fn=<SubBackward0>)
Episode: 1899 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1900 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1901 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1902 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1903 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1904 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1905 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1906 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1907 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1908 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1909 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1910 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1911 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9741, grad_fn=<SubBackward0>)
Episode: 1912 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9081, grad_fn=<SubBackward0>)
Episode: 1913 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3486, grad_fn=<SubBackward0>)
Episode: 1914 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1915 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1916 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1917 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9441, grad_fn=<SubBackward0>)
Episode: 1918 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1919 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1920 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1921 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1922 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1923 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1924 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1925 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1926 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1927 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1928 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1929 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8592, grad_fn=<SubBackward0>)
Episode: 1930 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1931 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9814, grad_fn=<SubBackward0>)
Episode: 1932 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1933 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1934 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1935 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1936 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1937 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1938 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9186, grad_fn=<SubBackward0>)
Episode: 1939 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1940 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1941 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 1942 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1277, grad_fn=<SubBackward0>)
Episode: 1943 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1944 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1945 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 1946 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 1947 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 1948 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1949 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 1950 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 1951 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2398, grad_fn=<SubBackward0>)
Episode: 1952 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6733, grad_fn=<SubBackward0>)
Episode: 1953 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1954 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1955 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1956 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1957 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7937, grad_fn=<SubBackward0>)
Episode: 1958 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1959 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 1960 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1961 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 1962 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8483, grad_fn=<SubBackward0>)
Episode: 1963 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 1964 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 1965 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2626, grad_fn=<SubBackward0>)
Episode: 1966 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1967 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1968 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1969 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9027, grad_fn=<SubBackward0>)
Episode: 1970 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 1971 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 1972 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 1973 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1243, grad_fn=<SubBackward0>)
Episode: 1974 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9764, grad_fn=<SubBackward0>)
Episode: 1975 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1976 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7446, grad_fn=<SubBackward0>)
Episode: 1977 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1978 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 1979 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 1980 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 1981 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1982 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0442, grad_fn=<SubBackward0>)
Episode: 1983 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9056, grad_fn=<SubBackward0>)
Episode: 1984 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 1985 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1986 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 1987 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 1988 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1989 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1990 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1991 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 1992 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8309, grad_fn=<SubBackward0>)
Episode: 1993 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1994 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1995 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1996 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1997 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7310, grad_fn=<SubBackward0>)
Episode: 1998 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1999 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 2000 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 2001 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1238, grad_fn=<SubBackward0>)
Episode: 2002 | Train Reward: tensor([1.]) | Train Loss: tensor(0.7126, grad_fn=<SubBackward0>)
Episode: 2003 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 2004 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 2005 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6417, grad_fn=<SubBackward0>)
Episode: 2006 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 2007 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1205, grad_fn=<SubBackward0>)
Episode: 2008 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 2009 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 2010 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1205, grad_fn=<SubBackward0>)
Episode: 2011 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8668, grad_fn=<SubBackward0>)
Episode: 2012 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1168, grad_fn=<SubBackward0>)
Episode: 2013 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 2014 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1218, grad_fn=<SubBackward0>)
Episode: 2015 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1218, grad_fn=<SubBackward0>)
Episode: 2016 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1199, grad_fn=<SubBackward0>)
Episode: 2017 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3041, grad_fn=<SubBackward0>)
Episode: 2018 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1221, grad_fn=<SubBackward0>)
Episode: 2019 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1209, grad_fn=<SubBackward0>)
Episode: 2020 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 2021 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.6220, grad_fn=<SubBackward0>)
Episode: 2022 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1238, grad_fn=<SubBackward0>)
Episode: 2023 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.5914, grad_fn=<SubBackward0>)
Episode: 2024 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1214, grad_fn=<SubBackward0>)
Episode: 2025 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 2026 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 2027 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1197, grad_fn=<SubBackward0>)
Episode: 2028 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1235, grad_fn=<SubBackward0>)
Episode: 2029 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1214, grad_fn=<SubBackward0>)
Episode: 2030 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 2031 | Train Reward: tensor([1.]) | Train Loss: tensor(1.4553, grad_fn=<SubBackward0>)
Episode: 2032 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1214, grad_fn=<SubBackward0>)
Episode: 2033 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1213, grad_fn=<SubBackward0>)
Episode: 2034 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2210, grad_fn=<SubBackward0>)
Episode: 2035 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 2036 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 2037 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 2038 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1228, grad_fn=<SubBackward0>)
Episode: 2039 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 2040 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 2041 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1237, grad_fn=<SubBackward0>)
Episode: 2042 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.4782, grad_fn=<SubBackward0>)
Episode: 2043 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 2044 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 2045 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 2046 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2451, grad_fn=<SubBackward0>)
Episode: 2047 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1243, grad_fn=<SubBackward0>)
Episode: 2048 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 2049 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2050 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 2051 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 2052 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 2053 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 2054 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 2055 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8068, grad_fn=<SubBackward0>)
Episode: 2056 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 2057 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 2058 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 2059 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7512, grad_fn=<SubBackward0>)
Episode: 2060 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 2061 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 2062 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 2063 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 2064 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1198, grad_fn=<SubBackward0>)
Episode: 2065 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 2066 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1224, grad_fn=<SubBackward0>)
Episode: 2067 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8832, grad_fn=<SubBackward0>)
Episode: 2068 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1243, grad_fn=<SubBackward0>)
Episode: 2069 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2070 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 2071 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 2072 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 2073 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 2074 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 2075 | Train Reward: tensor([1.]) | Train Loss: tensor(1.4116, grad_fn=<SubBackward0>)
Episode: 2076 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6820, grad_fn=<SubBackward0>)
Episode: 2077 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 2078 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 2079 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2080 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 2081 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1654, grad_fn=<SubBackward0>)
Episode: 2082 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 2083 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 2084 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 2085 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 2086 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8169, grad_fn=<SubBackward0>)
Episode: 2087 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9319, grad_fn=<SubBackward0>)
Episode: 2088 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 2089 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 2090 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 2091 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2092 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 2093 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 2094 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 2095 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 2096 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 2097 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2098 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2099 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 2100 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2101 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 2102 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 2103 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 2104 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 2105 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 2106 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9156, grad_fn=<SubBackward0>)
Episode: 2107 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7649, grad_fn=<SubBackward0>)
Episode: 2108 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 2109 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 2110 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2111 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 2112 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 2113 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 2114 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 2115 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 2116 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 2117 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 2118 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2119 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 2120 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 2121 | Train Reward: tensor([1.]) | Train Loss: tensor(0.7818, grad_fn=<SubBackward0>)
Episode: 2122 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 2123 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 2124 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 2125 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8939, grad_fn=<SubBackward0>)
Episode: 2126 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 2127 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 2128 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 2129 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2130 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0360, grad_fn=<SubBackward0>)
Episode: 2131 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2410, grad_fn=<SubBackward0>)
Episode: 2132 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 2133 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1205, grad_fn=<SubBackward0>)
