{'output_directory': 'save_folder', 'grid_size': 4, 'min_other_objects': 4, 'max_objects': 4, 'min_object_size': 1, 'max_object_size': 4, 'other_objects_sample_percentage': 0.3, 'obstacles_flag': False, 'num_obstacles': 5, 'enable_maze': False, 'maze_complexity': 0, 'maze_density': 0, 'type_grammar': 'simple_intrans', 'intransitive_verbs': 'walk', 'transitive_verbs': 'push,pull,pickup,drop', 'nouns': 'circle,square,cylinder,diamond', 'color_adjectives': 'red,blue,yellow,green', 'size_adjectives': '', 'keep_fixed_weights': True, 'all_light': True, 'num_episodes': 300000, 'episode_len': 10, 'grid_input_type': 'vector', 'comm_type': 'continuous', 'comm_setting': 'cheap_talk', 'temp': 1, 'lights_out': False, 'render_episode': False, 'wait_time': 0.3}
Episode: 1 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 2 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 3 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 4 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 5 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 6 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1508, grad_fn=<SubBackward0>)
Episode: 7 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 8 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 9 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 10 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 11 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 12 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 13 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8145, grad_fn=<SubBackward0>)
Episode: 14 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1102, grad_fn=<SubBackward0>)
Episode: 15 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 16 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 17 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 18 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 19 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 20 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1744, grad_fn=<SubBackward0>)
Episode: 21 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7918, grad_fn=<SubBackward0>)
Episode: 22 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 23 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 24 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 25 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 26 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 27 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 28 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0753, grad_fn=<SubBackward0>)
Episode: 29 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 30 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7929, grad_fn=<SubBackward0>)
Episode: 31 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 32 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 33 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 34 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 35 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 36 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 37 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 38 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 39 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 40 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 41 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 42 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 43 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 44 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 45 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2213, grad_fn=<SubBackward0>)
Episode: 46 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 47 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 48 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 49 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 50 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1448, grad_fn=<SubBackward0>)
Episode: 51 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 52 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1380, grad_fn=<SubBackward0>)
Episode: 53 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9596, grad_fn=<SubBackward0>)
Episode: 54 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 55 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 56 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 57 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 58 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 59 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1688, grad_fn=<SubBackward0>)
Episode: 60 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 61 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 62 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 63 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 64 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 65 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 66 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 67 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1731, grad_fn=<SubBackward0>)
Episode: 68 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 69 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1695, grad_fn=<SubBackward0>)
Episode: 70 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1638, grad_fn=<SubBackward0>)
Episode: 71 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 72 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 73 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 74 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0922, grad_fn=<SubBackward0>)
Episode: 75 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 76 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 77 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 78 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 79 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 80 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 81 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0022, grad_fn=<SubBackward0>)
Episode: 82 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 83 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 84 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 85 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 86 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 87 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 88 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8847, grad_fn=<SubBackward0>)
Episode: 89 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 90 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 91 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 92 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 93 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 94 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 95 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 96 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 97 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3737, grad_fn=<SubBackward0>)
Episode: 98 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0582, grad_fn=<SubBackward0>)
Episode: 99 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 100 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 101 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 102 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 103 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 104 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 105 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 106 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9422, grad_fn=<SubBackward0>)
Episode: 107 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 108 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 109 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 110 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 111 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8722, grad_fn=<SubBackward0>)
Episode: 112 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 113 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 114 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 115 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 116 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 117 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 118 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0993, grad_fn=<SubBackward0>)
Episode: 119 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 120 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 121 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 122 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 123 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8273, grad_fn=<SubBackward0>)
Episode: 124 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 125 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8839, grad_fn=<SubBackward0>)
Episode: 126 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 127 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 128 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 129 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 130 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 131 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 132 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 133 | Train Reward: tensor([1.]) | Train Loss: tensor(1.5873, grad_fn=<SubBackward0>)
Episode: 134 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 135 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 136 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 137 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 138 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 139 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 140 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 141 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 142 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 143 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 144 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 145 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 146 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 147 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 148 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7505, grad_fn=<SubBackward0>)
Episode: 149 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 150 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 151 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 152 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9313, grad_fn=<SubBackward0>)
Episode: 153 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 154 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 155 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 156 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 157 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 158 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0125, grad_fn=<SubBackward0>)
Episode: 159 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 160 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 161 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 162 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 163 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9920, grad_fn=<SubBackward0>)
Episode: 164 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 165 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 166 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 167 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 168 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7837, grad_fn=<SubBackward0>)
Episode: 169 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 170 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 171 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 172 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 173 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 174 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 175 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 176 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 177 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 178 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 179 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 180 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 181 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.8950, grad_fn=<SubBackward0>)
Episode: 182 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 183 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 184 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 185 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 186 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 187 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 188 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 189 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 190 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 191 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0542, grad_fn=<SubBackward0>)
Episode: 192 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 193 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 194 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 195 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 196 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8547, grad_fn=<SubBackward0>)
Episode: 197 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.8966, grad_fn=<SubBackward0>)
Episode: 198 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 199 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0472, grad_fn=<SubBackward0>)
Episode: 200 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 201 | Train Reward: tensor([1.]) | Train Loss: tensor(1.5391, grad_fn=<SubBackward0>)
Episode: 202 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 203 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 204 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7521, grad_fn=<SubBackward0>)
Episode: 205 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 206 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 207 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 208 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 209 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 210 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 211 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 212 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 213 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 214 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0887, grad_fn=<SubBackward0>)
Episode: 215 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 216 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 217 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 218 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 219 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 220 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 221 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 222 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 223 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 224 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 225 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 226 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 227 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 228 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1882, grad_fn=<SubBackward0>)
Episode: 229 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 230 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 231 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 232 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 233 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 234 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 235 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8410, grad_fn=<SubBackward0>)
Episode: 236 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1604, grad_fn=<SubBackward0>)
Episode: 237 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7717, grad_fn=<SubBackward0>)
Episode: 238 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 239 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 240 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 241 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 242 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 243 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 244 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8162, grad_fn=<SubBackward0>)
Episode: 245 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 246 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 247 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 248 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 249 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 250 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 251 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8146, grad_fn=<SubBackward0>)
Episode: 252 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 253 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9073, grad_fn=<SubBackward0>)
Episode: 254 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 255 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 256 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 257 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 258 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 259 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 260 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3411, grad_fn=<SubBackward0>)
Episode: 261 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 262 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 263 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 264 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 265 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8993, grad_fn=<SubBackward0>)
Episode: 266 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 267 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 268 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 269 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 270 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 271 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 272 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 273 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 274 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7160, grad_fn=<SubBackward0>)
Episode: 275 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 276 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 277 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 278 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9182, grad_fn=<SubBackward0>)
Episode: 279 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 280 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1280, grad_fn=<SubBackward0>)
Episode: 281 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 282 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 283 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 284 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 285 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 286 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 287 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 288 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 289 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 290 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 291 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 292 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 293 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 294 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 295 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 296 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0862, grad_fn=<SubBackward0>)
Episode: 297 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 298 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 299 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 300 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 301 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 302 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 303 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 304 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 305 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 306 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8089, grad_fn=<SubBackward0>)
Episode: 307 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 308 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 309 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 310 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 311 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 312 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 313 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 314 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 315 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 316 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 317 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8938, grad_fn=<SubBackward0>)
Episode: 318 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 319 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 320 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 321 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 322 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9399, grad_fn=<SubBackward0>)
Episode: 323 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 324 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8663, grad_fn=<SubBackward0>)
Episode: 325 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 326 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 327 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 328 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 329 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 330 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 331 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 332 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 333 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 334 | Train Reward: tensor([1.]) | Train Loss: tensor(1.4376, grad_fn=<SubBackward0>)
Episode: 335 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3452, grad_fn=<SubBackward0>)
Episode: 336 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 337 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 338 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 339 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 340 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8562, grad_fn=<SubBackward0>)
Episode: 341 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 342 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 343 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 344 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0273, grad_fn=<SubBackward0>)
Episode: 345 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 346 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 347 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 348 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 349 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7363, grad_fn=<SubBackward0>)
Episode: 350 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 351 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.4458, grad_fn=<SubBackward0>)
Episode: 352 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 353 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 354 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 355 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 356 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2886, grad_fn=<SubBackward0>)
Episode: 357 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 358 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 359 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 360 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 361 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 362 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 363 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 364 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 365 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 366 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9518, grad_fn=<SubBackward0>)
Episode: 367 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1379, grad_fn=<SubBackward0>)
Episode: 368 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1931, grad_fn=<SubBackward0>)
Episode: 369 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 370 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 371 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9588, grad_fn=<SubBackward0>)
Episode: 372 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 373 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 374 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 375 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 376 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 377 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 378 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 379 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2039, grad_fn=<SubBackward0>)
Episode: 380 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1495, grad_fn=<SubBackward0>)
Episode: 381 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 382 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 383 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 384 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 385 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 386 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 387 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 388 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 389 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 390 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 391 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 392 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 393 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7848, grad_fn=<SubBackward0>)
Episode: 394 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0028, grad_fn=<SubBackward0>)
Episode: 395 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 396 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 397 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 398 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1441, grad_fn=<SubBackward0>)
Episode: 399 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 400 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 401 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 402 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 403 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 404 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 405 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 406 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 407 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 408 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 409 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 410 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 411 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 412 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 413 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 414 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 415 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 416 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 417 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 418 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 419 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1792, grad_fn=<SubBackward0>)
Episode: 420 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 421 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 422 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 423 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 424 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 425 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 426 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 427 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 428 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 429 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 430 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 431 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 432 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 433 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 434 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0232, grad_fn=<SubBackward0>)
Episode: 435 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 436 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 437 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 438 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 439 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 440 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 441 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8340, grad_fn=<SubBackward0>)
Episode: 442 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 443 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 444 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 445 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 446 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3516, grad_fn=<SubBackward0>)
Episode: 447 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 448 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 449 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 450 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 451 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 452 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9394, grad_fn=<SubBackward0>)
Episode: 453 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 454 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 455 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 456 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 457 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1561, grad_fn=<SubBackward0>)
Episode: 458 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 459 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 460 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 461 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 462 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 463 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9358, grad_fn=<SubBackward0>)
Episode: 464 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.2468, grad_fn=<SubBackward0>)
Episode: 465 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 466 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 467 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0871, grad_fn=<SubBackward0>)
Episode: 468 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0845, grad_fn=<SubBackward0>)
Episode: 469 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 470 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8775, grad_fn=<SubBackward0>)
Episode: 471 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0763, grad_fn=<SubBackward0>)
Episode: 472 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 473 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 474 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 475 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 476 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 477 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 478 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 479 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 480 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 481 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 482 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9338, grad_fn=<SubBackward0>)
Episode: 483 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9285, grad_fn=<SubBackward0>)
Episode: 484 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 485 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 486 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 487 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9245, grad_fn=<SubBackward0>)
Episode: 488 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9720, grad_fn=<SubBackward0>)
Episode: 489 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 490 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 491 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 492 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 493 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 494 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 495 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 496 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 497 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 498 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0353, grad_fn=<SubBackward0>)
Episode: 499 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 500 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 501 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 502 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9946, grad_fn=<SubBackward0>)
Episode: 503 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 504 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 505 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 506 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 507 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 508 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 509 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 510 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1004, grad_fn=<SubBackward0>)
Episode: 511 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 512 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 513 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 514 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 515 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 516 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 517 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9980, grad_fn=<SubBackward0>)
Episode: 518 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7935, grad_fn=<SubBackward0>)
Episode: 519 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 520 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 521 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 522 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 523 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 524 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 525 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 526 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 527 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 528 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 529 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 530 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 531 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8147, grad_fn=<SubBackward0>)
Episode: 532 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 533 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 534 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 535 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 536 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 537 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 538 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 539 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 540 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0672, grad_fn=<SubBackward0>)
Episode: 541 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7457, grad_fn=<SubBackward0>)
Episode: 542 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 543 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 544 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 545 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 546 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 547 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 548 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 549 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3749, grad_fn=<SubBackward0>)
Episode: 550 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 551 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 552 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 553 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 554 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 555 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9922, grad_fn=<SubBackward0>)
Episode: 556 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 557 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8671, grad_fn=<SubBackward0>)
Episode: 558 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 559 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 560 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 561 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3138, grad_fn=<SubBackward0>)
Episode: 562 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 563 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 564 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 565 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.8678, grad_fn=<SubBackward0>)
Episode: 566 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 567 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 568 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 569 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 570 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 571 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 572 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 573 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8696, grad_fn=<SubBackward0>)
Episode: 574 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 575 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 576 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 577 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 578 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0852, grad_fn=<SubBackward0>)
Episode: 579 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9131, grad_fn=<SubBackward0>)
Episode: 580 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 581 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 582 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 583 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 584 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 585 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 586 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 587 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 588 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 589 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 590 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 591 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0905, grad_fn=<SubBackward0>)
Episode: 592 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9702, grad_fn=<SubBackward0>)
Episode: 593 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9662, grad_fn=<SubBackward0>)
Episode: 594 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7659, grad_fn=<SubBackward0>)
Episode: 595 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 596 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 597 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 598 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 599 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 600 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9313, grad_fn=<SubBackward0>)
Episode: 601 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 602 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 603 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 604 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 605 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 606 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 607 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 608 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 609 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 610 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 611 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 612 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 613 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 614 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 615 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 616 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 617 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 618 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8979, grad_fn=<SubBackward0>)
Episode: 619 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 620 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 621 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0938, grad_fn=<SubBackward0>)
Episode: 622 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 623 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 624 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.8594, grad_fn=<SubBackward0>)
Episode: 625 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 626 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 627 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 628 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 629 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9643, grad_fn=<SubBackward0>)
Episode: 630 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 631 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 632 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 633 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 634 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8616, grad_fn=<SubBackward0>)
Episode: 635 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0897, grad_fn=<SubBackward0>)
Episode: 636 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 637 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 638 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9368, grad_fn=<SubBackward0>)
Episode: 639 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 640 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 641 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 642 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 643 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 644 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 645 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 646 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0895, grad_fn=<SubBackward0>)
Episode: 647 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 648 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 649 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6249, grad_fn=<SubBackward0>)
Episode: 650 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7929, grad_fn=<SubBackward0>)
Episode: 651 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 652 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 653 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1223, grad_fn=<SubBackward0>)
Episode: 654 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3651, grad_fn=<SubBackward0>)
Episode: 655 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1223, grad_fn=<SubBackward0>)
Episode: 656 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1235, grad_fn=<SubBackward0>)
Episode: 657 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1238, grad_fn=<SubBackward0>)
Episode: 658 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6300, grad_fn=<SubBackward0>)
Episode: 659 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1230, grad_fn=<SubBackward0>)
Episode: 660 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 661 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 662 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 663 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1241, grad_fn=<SubBackward0>)
Episode: 664 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7123, grad_fn=<SubBackward0>)
Episode: 665 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 666 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 667 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1233, grad_fn=<SubBackward0>)
Episode: 668 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1233, grad_fn=<SubBackward0>)
Episode: 669 | Train Reward: tensor([1.]) | Train Loss: tensor(0.7752, grad_fn=<SubBackward0>)
Episode: 670 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 671 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 672 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 673 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7601, grad_fn=<SubBackward0>)
Episode: 674 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 675 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1195, grad_fn=<SubBackward0>)
Episode: 676 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1232, grad_fn=<SubBackward0>)
Episode: 677 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1202, grad_fn=<SubBackward0>)
Episode: 678 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 679 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1232, grad_fn=<SubBackward0>)
Episode: 680 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 681 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 682 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2848, grad_fn=<SubBackward0>)
Episode: 683 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 684 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 685 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 686 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 687 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2536, grad_fn=<SubBackward0>)
Episode: 688 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 689 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8060, grad_fn=<SubBackward0>)
Episode: 690 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 691 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1237, grad_fn=<SubBackward0>)
Episode: 692 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1228, grad_fn=<SubBackward0>)
Episode: 693 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 694 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1238, grad_fn=<SubBackward0>)
Episode: 695 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 696 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 697 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 698 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7281, grad_fn=<SubBackward0>)
Episode: 699 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1206, grad_fn=<SubBackward0>)
Episode: 700 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7430, grad_fn=<SubBackward0>)
Episode: 701 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1224, grad_fn=<SubBackward0>)
Episode: 702 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1231, grad_fn=<SubBackward0>)
Episode: 703 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1224, grad_fn=<SubBackward0>)
Episode: 704 | Train Reward: tensor([1.]) | Train Loss: tensor(0.7921, grad_fn=<SubBackward0>)
Episode: 705 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 706 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1225, grad_fn=<SubBackward0>)
Episode: 707 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1220, grad_fn=<SubBackward0>)
Episode: 708 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1209, grad_fn=<SubBackward0>)
Episode: 709 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1202, grad_fn=<SubBackward0>)
Episode: 710 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 711 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1209, grad_fn=<SubBackward0>)
Episode: 712 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1204, grad_fn=<SubBackward0>)
Episode: 713 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1220, grad_fn=<SubBackward0>)
Episode: 714 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1205, grad_fn=<SubBackward0>)
Episode: 715 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6778, grad_fn=<SubBackward0>)
Episode: 716 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1207, grad_fn=<SubBackward0>)
Episode: 717 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1210, grad_fn=<SubBackward0>)
Episode: 718 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6126, grad_fn=<SubBackward0>)
Episode: 719 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1207, grad_fn=<SubBackward0>)
Episode: 720 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7481, grad_fn=<SubBackward0>)
Episode: 721 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1209, grad_fn=<SubBackward0>)
Episode: 722 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1215, grad_fn=<SubBackward0>)
Episode: 723 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1219, grad_fn=<SubBackward0>)
Episode: 724 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1205, grad_fn=<SubBackward0>)
Episode: 725 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1188, grad_fn=<SubBackward0>)
Episode: 726 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1203, grad_fn=<SubBackward0>)
Episode: 727 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1226, grad_fn=<SubBackward0>)
Episode: 728 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1220, grad_fn=<SubBackward0>)
Episode: 729 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7714, grad_fn=<SubBackward0>)
Episode: 730 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1212, grad_fn=<SubBackward0>)
Episode: 731 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.6859, grad_fn=<SubBackward0>)
Episode: 732 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1224, grad_fn=<SubBackward0>)
Episode: 733 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1208, grad_fn=<SubBackward0>)
Episode: 734 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1204, grad_fn=<SubBackward0>)
Episode: 735 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1202, grad_fn=<SubBackward0>)
Episode: 736 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0501, grad_fn=<SubBackward0>)
Episode: 737 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8242, grad_fn=<SubBackward0>)
Episode: 738 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1237, grad_fn=<SubBackward0>)
Episode: 739 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1231, grad_fn=<SubBackward0>)
Episode: 740 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1219, grad_fn=<SubBackward0>)
Episode: 741 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8005, grad_fn=<SubBackward0>)
Episode: 742 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1235, grad_fn=<SubBackward0>)
Episode: 743 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 744 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1225, grad_fn=<SubBackward0>)
Episode: 745 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 746 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 747 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 748 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 749 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1225, grad_fn=<SubBackward0>)
Episode: 750 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7865, grad_fn=<SubBackward0>)
Episode: 751 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 752 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 753 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 754 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 755 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 756 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 757 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 758 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 759 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9945, grad_fn=<SubBackward0>)
Episode: 760 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 761 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 762 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 763 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 764 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 765 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 766 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8166, grad_fn=<SubBackward0>)
Episode: 767 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 768 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 769 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 770 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.8910, grad_fn=<SubBackward0>)
Episode: 771 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 772 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 773 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9985, grad_fn=<SubBackward0>)
Episode: 774 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 775 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9016, grad_fn=<SubBackward0>)
Episode: 776 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 777 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 778 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 779 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 780 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 781 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 782 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 783 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 784 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0613, grad_fn=<SubBackward0>)
Episode: 785 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 786 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 787 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 788 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 789 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 790 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 791 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 792 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0981, grad_fn=<SubBackward0>)
Episode: 793 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 794 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 795 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 796 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 797 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 798 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 799 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 800 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0406, grad_fn=<SubBackward0>)
Episode: 801 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 802 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9136, grad_fn=<SubBackward0>)
Episode: 803 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 804 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 805 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9892, grad_fn=<SubBackward0>)
Episode: 806 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 807 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 808 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 809 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 810 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 811 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 812 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2626, grad_fn=<SubBackward0>)
Episode: 813 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 814 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0388, grad_fn=<SubBackward0>)
Episode: 815 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 816 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9821, grad_fn=<SubBackward0>)
Episode: 817 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9611, grad_fn=<SubBackward0>)
Episode: 818 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 819 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 820 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 821 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1326, grad_fn=<SubBackward0>)
Episode: 822 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 823 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 824 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 825 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 826 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 827 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 828 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 829 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 830 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2168, grad_fn=<SubBackward0>)
Episode: 831 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 832 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 833 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2071, grad_fn=<SubBackward0>)
Episode: 834 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 835 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 836 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1807, grad_fn=<SubBackward0>)
Episode: 837 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 838 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 839 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 840 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 841 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8264, grad_fn=<SubBackward0>)
Episode: 842 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 843 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0932, grad_fn=<SubBackward0>)
Episode: 844 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3048, grad_fn=<SubBackward0>)
Episode: 845 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 846 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 847 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 848 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 849 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 850 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 851 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0490, grad_fn=<SubBackward0>)
Episode: 852 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 853 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1606, grad_fn=<SubBackward0>)
Episode: 854 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1595, grad_fn=<SubBackward0>)
Episode: 855 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0535, grad_fn=<SubBackward0>)
Episode: 856 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7363, grad_fn=<SubBackward0>)
Episode: 857 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7867, grad_fn=<SubBackward0>)
Episode: 858 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 859 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 860 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 861 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 862 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 863 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 864 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 865 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 866 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8843, grad_fn=<SubBackward0>)
Episode: 867 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 868 | Train Reward: tensor([1.]) | Train Loss: tensor(1.4239, grad_fn=<SubBackward0>)
Episode: 869 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 870 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 871 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2382, grad_fn=<SubBackward0>)
Episode: 872 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9857, grad_fn=<SubBackward0>)
Episode: 873 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3760, grad_fn=<SubBackward0>)
Episode: 874 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 875 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7958, grad_fn=<SubBackward0>)
Episode: 876 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 877 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 878 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8628, grad_fn=<SubBackward0>)
Episode: 879 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 880 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 881 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 882 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 883 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 884 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 885 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 886 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 887 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 888 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 889 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 890 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 891 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 892 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0039, grad_fn=<SubBackward0>)
Episode: 893 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9653, grad_fn=<SubBackward0>)
Episode: 894 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 895 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 896 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8040, grad_fn=<SubBackward0>)
Episode: 897 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 898 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6168, grad_fn=<SubBackward0>)
Episode: 899 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 900 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2906, grad_fn=<SubBackward0>)
Episode: 901 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 902 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 903 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 904 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 905 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 906 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1235, grad_fn=<SubBackward0>)
Episode: 907 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9167, grad_fn=<SubBackward0>)
Episode: 908 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6475, grad_fn=<SubBackward0>)
Episode: 909 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 910 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 911 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 912 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1366, grad_fn=<SubBackward0>)
Episode: 913 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 914 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 915 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7746, grad_fn=<SubBackward0>)
Episode: 916 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9371, grad_fn=<SubBackward0>)
Episode: 917 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1221, grad_fn=<SubBackward0>)
Episode: 918 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1065, grad_fn=<SubBackward0>)
Episode: 919 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 920 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 921 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6695, grad_fn=<SubBackward0>)
Episode: 922 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1223, grad_fn=<SubBackward0>)
Episode: 923 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1236, grad_fn=<SubBackward0>)
Episode: 924 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1217, grad_fn=<SubBackward0>)
Episode: 925 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1230, grad_fn=<SubBackward0>)
Episode: 926 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1200, grad_fn=<SubBackward0>)
Episode: 927 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1213, grad_fn=<SubBackward0>)
Episode: 928 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 929 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1214, grad_fn=<SubBackward0>)
Episode: 930 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8527, grad_fn=<SubBackward0>)
Episode: 931 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1207, grad_fn=<SubBackward0>)
Episode: 932 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1133, grad_fn=<SubBackward0>)
Episode: 933 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 934 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8232, grad_fn=<SubBackward0>)
Episode: 935 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8093, grad_fn=<SubBackward0>)
Episode: 936 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1180, grad_fn=<SubBackward0>)
Episode: 937 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1205, grad_fn=<SubBackward0>)
Episode: 938 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1223, grad_fn=<SubBackward0>)
Episode: 939 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 940 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 941 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9351, grad_fn=<SubBackward0>)
Episode: 942 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 943 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 944 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 945 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 946 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.4224, grad_fn=<SubBackward0>)
Episode: 947 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 948 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 949 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8968, grad_fn=<SubBackward0>)
Episode: 950 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 951 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 952 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 953 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 954 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9109, grad_fn=<SubBackward0>)
Episode: 955 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 956 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 957 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 958 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 959 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8369, grad_fn=<SubBackward0>)
Episode: 960 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 961 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 962 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 963 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 964 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 965 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 966 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 967 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 968 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 969 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 970 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 971 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 972 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0254, grad_fn=<SubBackward0>)
Episode: 973 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 974 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 975 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0690, grad_fn=<SubBackward0>)
Episode: 976 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 977 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 978 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 979 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 980 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 981 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3091, grad_fn=<SubBackward0>)
Episode: 982 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 983 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 984 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 985 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 986 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 987 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 988 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 989 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 990 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 991 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 992 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 993 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 994 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 995 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 996 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 997 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 998 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8820, grad_fn=<SubBackward0>)
Episode: 999 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 1000 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1001 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1002 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2827, grad_fn=<SubBackward0>)
Episode: 1003 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1004 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1005 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1006 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1007 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1008 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8762, grad_fn=<SubBackward0>)
Episode: 1009 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9537, grad_fn=<SubBackward0>)
Episode: 1010 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1011 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1012 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1013 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1014 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1015 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9629, grad_fn=<SubBackward0>)
Episode: 1016 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1017 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1018 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1019 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1020 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1021 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0004, grad_fn=<SubBackward0>)
Episode: 1022 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1023 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6368, grad_fn=<SubBackward0>)
Episode: 1024 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1025 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1026 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9510, grad_fn=<SubBackward0>)
Episode: 1027 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1028 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1029 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1030 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1105, grad_fn=<SubBackward0>)
Episode: 1031 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 1032 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1033 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1034 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2935, grad_fn=<SubBackward0>)
Episode: 1035 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 1036 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1037 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 1038 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 1039 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1040 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 1041 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 1042 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 1043 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 1044 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 1045 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1046 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 1047 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 1048 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1049 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1050 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1051 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1052 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1053 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1054 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1055 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1056 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1057 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8930, grad_fn=<SubBackward0>)
Episode: 1058 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6549, grad_fn=<SubBackward0>)
Episode: 1059 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1060 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1061 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1062 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 1063 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1064 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1065 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 1066 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 1067 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1068 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1069 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1070 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1071 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1072 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1073 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1074 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9823, grad_fn=<SubBackward0>)
Episode: 1075 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1076 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8937, grad_fn=<SubBackward0>)
Episode: 1077 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1078 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1079 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1080 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1081 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1082 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 1083 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1084 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1085 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1086 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 1087 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1088 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 1089 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 1090 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1091 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1092 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2033, grad_fn=<SubBackward0>)
Episode: 1093 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8750, grad_fn=<SubBackward0>)
Episode: 1094 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 1095 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3496, grad_fn=<SubBackward0>)
Episode: 1096 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1319, grad_fn=<SubBackward0>)
Episode: 1097 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7098, grad_fn=<SubBackward0>)
Episode: 1098 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1099 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1100 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 1101 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8897, grad_fn=<SubBackward0>)
Episode: 1102 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1103 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1104 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1105 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 1106 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8302, grad_fn=<SubBackward0>)
Episode: 1107 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.5800, grad_fn=<SubBackward0>)
Episode: 1108 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1233, grad_fn=<SubBackward0>)
Episode: 1109 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1110 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 1111 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1112 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 1113 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 1114 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 1115 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 1116 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 1117 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 1118 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 1119 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8474, grad_fn=<SubBackward0>)
Episode: 1120 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1121 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1122 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 1123 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1124 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 1125 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 1126 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1127 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1128 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0843, grad_fn=<SubBackward0>)
Episode: 1129 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 1130 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1131 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1132 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 1133 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2022, grad_fn=<SubBackward0>)
Episode: 1134 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7043, grad_fn=<SubBackward0>)
Episode: 1135 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1136 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1137 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1138 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1139 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1140 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1141 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1142 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 1143 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1144 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6863, grad_fn=<SubBackward0>)
Episode: 1145 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 1146 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1147 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1148 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8363, grad_fn=<SubBackward0>)
Episode: 1149 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1150 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1151 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1152 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1153 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1154 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1155 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1156 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1157 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1158 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1159 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6675, grad_fn=<SubBackward0>)
Episode: 1160 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6245, grad_fn=<SubBackward0>)
Episode: 1161 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1162 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9104, grad_fn=<SubBackward0>)
Episode: 1163 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1164 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1165 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1166 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1167 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0010, grad_fn=<SubBackward0>)
Episode: 1168 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1169 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1170 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1171 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1172 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1173 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1174 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8215, grad_fn=<SubBackward0>)
Episode: 1175 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1176 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1177 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1178 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1179 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1180 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1181 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1182 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2156, grad_fn=<SubBackward0>)
Episode: 1183 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1184 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1185 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1186 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1187 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1188 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1189 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1190 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8374, grad_fn=<SubBackward0>)
Episode: 1191 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1192 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1193 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 1194 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1195 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1196 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1197 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1198 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1199 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1200 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1201 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1202 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1203 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1204 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1205 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1206 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1207 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1208 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0552, grad_fn=<SubBackward0>)
Episode: 1209 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1210 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1211 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8162, grad_fn=<SubBackward0>)
Episode: 1212 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1213 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8175, grad_fn=<SubBackward0>)
Episode: 1214 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1215 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1216 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1217 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1780, grad_fn=<SubBackward0>)
Episode: 1218 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1219 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1220 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1221 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1222 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1223 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 1224 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1135, grad_fn=<SubBackward0>)
Episode: 1225 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1226 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 1227 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1228 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 1229 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 1230 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0320, grad_fn=<SubBackward0>)
Episode: 1231 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 1232 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7601, grad_fn=<SubBackward0>)
Episode: 1233 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 1234 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9113, grad_fn=<SubBackward0>)
Episode: 1235 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9245, grad_fn=<SubBackward0>)
Episode: 1236 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7535, grad_fn=<SubBackward0>)
Episode: 1237 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 1238 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1239 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0081, grad_fn=<SubBackward0>)
Episode: 1240 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9925, grad_fn=<SubBackward0>)
Episode: 1241 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 1242 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1223, grad_fn=<SubBackward0>)
Episode: 1243 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1210, grad_fn=<SubBackward0>)
Episode: 1244 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1197, grad_fn=<SubBackward0>)
Episode: 1245 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1214, grad_fn=<SubBackward0>)
Episode: 1246 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1243, grad_fn=<SubBackward0>)
Episode: 1247 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1317, grad_fn=<SubBackward0>)
Episode: 1248 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 1249 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 1250 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1223, grad_fn=<SubBackward0>)
Episode: 1251 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 1252 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 1253 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1222, grad_fn=<SubBackward0>)
Episode: 1254 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1215, grad_fn=<SubBackward0>)
Episode: 1255 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1212, grad_fn=<SubBackward0>)
Episode: 1256 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0024, grad_fn=<SubBackward0>)
Episode: 1257 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1241, grad_fn=<SubBackward0>)
Episode: 1258 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 1259 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 1260 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 1261 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1230, grad_fn=<SubBackward0>)
Episode: 1262 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0757, grad_fn=<SubBackward0>)
Episode: 1263 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1027, grad_fn=<SubBackward0>)
Episode: 1264 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 1265 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 1266 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1267 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 1268 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 1269 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1233, grad_fn=<SubBackward0>)
Episode: 1270 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6748, grad_fn=<SubBackward0>)
Episode: 1271 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0893, grad_fn=<SubBackward0>)
Episode: 1272 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1273 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1274 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1275 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 1276 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2592, grad_fn=<SubBackward0>)
Episode: 1277 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1278 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1279 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1280 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1281 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1282 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1283 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0531, grad_fn=<SubBackward0>)
Episode: 1284 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1285 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0419, grad_fn=<SubBackward0>)
Episode: 1286 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1287 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1885, grad_fn=<SubBackward0>)
Episode: 1288 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7418, grad_fn=<SubBackward0>)
Episode: 1289 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1290 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9791, grad_fn=<SubBackward0>)
Episode: 1291 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 1292 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1293 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1294 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1295 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1296 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1297 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0117, grad_fn=<SubBackward0>)
Episode: 1298 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1299 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1300 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1301 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1302 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1303 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1304 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1305 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1306 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7857, grad_fn=<SubBackward0>)
Episode: 1307 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1308 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1309 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1310 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1311 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1312 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 1313 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1314 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1315 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1316 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0399, grad_fn=<SubBackward0>)
Episode: 1317 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 1318 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1319 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2539, grad_fn=<SubBackward0>)
Episode: 1320 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1321 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1322 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8624, grad_fn=<SubBackward0>)
Episode: 1323 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0345, grad_fn=<SubBackward0>)
Episode: 1324 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1325 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1326 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1327 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1328 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1329 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1330 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1331 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1332 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1333 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1334 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1335 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7223, grad_fn=<SubBackward0>)
Episode: 1336 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1337 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9239, grad_fn=<SubBackward0>)
Episode: 1338 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1339 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1340 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1341 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1342 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1343 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8928, grad_fn=<SubBackward0>)
Episode: 1344 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1345 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1346 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2107, grad_fn=<SubBackward0>)
Episode: 1347 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1348 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1349 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1350 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1351 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1352 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1353 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1304, grad_fn=<SubBackward0>)
Episode: 1354 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1355 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 1356 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1357 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1358 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7102, grad_fn=<SubBackward0>)
Episode: 1359 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1360 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1361 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1362 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1407, grad_fn=<SubBackward0>)
Episode: 1363 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1364 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1365 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1366 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2414, grad_fn=<SubBackward0>)
Episode: 1367 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1368 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1369 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1370 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 1371 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6629, grad_fn=<SubBackward0>)
Episode: 1372 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9447, grad_fn=<SubBackward0>)
Episode: 1373 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 1374 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1375 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1376 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 1377 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1378 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1379 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 1380 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 1381 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 1382 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 1383 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6936, grad_fn=<SubBackward0>)
Episode: 1384 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1385 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.5097, grad_fn=<SubBackward0>)
Episode: 1386 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7882, grad_fn=<SubBackward0>)
Episode: 1387 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1388 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 1389 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1390 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1391 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6947, grad_fn=<SubBackward0>)
Episode: 1392 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1393 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 1394 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1395 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1396 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1397 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1398 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1399 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1400 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2749, grad_fn=<SubBackward0>)
Episode: 1401 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1402 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1403 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1404 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1405 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.8640, grad_fn=<SubBackward0>)
Episode: 1406 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 1407 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9089, grad_fn=<SubBackward0>)
Episode: 1408 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1409 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 1410 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1411 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1412 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0014, grad_fn=<SubBackward0>)
Episode: 1413 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1414 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8581, grad_fn=<SubBackward0>)
Episode: 1415 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1416 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8469, grad_fn=<SubBackward0>)
Episode: 1417 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1418 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1419 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1420 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1421 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1422 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 1423 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8927, grad_fn=<SubBackward0>)
Episode: 1424 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1425 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1426 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1427 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1428 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1429 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 1430 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 1431 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 1432 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8671, grad_fn=<SubBackward0>)
Episode: 1433 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1434 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 1435 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1436 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1437 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 1438 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8784, grad_fn=<SubBackward0>)
Episode: 1439 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1440 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0598, grad_fn=<SubBackward0>)
Episode: 1441 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 1442 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1443 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 1444 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1445 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1446 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 1447 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1448 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1449 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7685, grad_fn=<SubBackward0>)
Episode: 1450 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 1451 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1452 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1453 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1454 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1455 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1456 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1457 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1458 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1459 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1460 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1461 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1462 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1463 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1464 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9278, grad_fn=<SubBackward0>)
Episode: 1465 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1466 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1467 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1468 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1469 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1470 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1471 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1472 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1473 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1474 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 1475 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1476 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1477 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1478 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1479 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1480 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1481 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1482 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9282, grad_fn=<SubBackward0>)
Episode: 1483 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1484 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 1485 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9741, grad_fn=<SubBackward0>)
Episode: 1486 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8861, grad_fn=<SubBackward0>)
Episode: 1487 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1488 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1489 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 1490 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1491 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1492 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1493 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1494 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8190, grad_fn=<SubBackward0>)
Episode: 1495 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1496 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 1497 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1498 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1984, grad_fn=<SubBackward0>)
Episode: 1499 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1500 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1501 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1502 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1503 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1504 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1505 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 1506 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0579, grad_fn=<SubBackward0>)
Episode: 1507 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1508 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1509 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 1510 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1511 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1512 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1513 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0479, grad_fn=<SubBackward0>)
Episode: 1514 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1515 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1516 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 1517 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1518 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1519 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0241, grad_fn=<SubBackward0>)
Episode: 1520 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 1521 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1522 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 1523 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1482, grad_fn=<SubBackward0>)
Episode: 1524 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1525 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1526 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 1527 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1845, grad_fn=<SubBackward0>)
Episode: 1528 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1529 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8318, grad_fn=<SubBackward0>)
Episode: 1530 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 1531 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1532 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1533 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1534 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1535 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 1536 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 1537 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7348, grad_fn=<SubBackward0>)
Episode: 1538 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 1539 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 1540 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 1541 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 1542 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 1543 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 1544 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 1545 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9702, grad_fn=<SubBackward0>)
Episode: 1546 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 1547 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 1548 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1549 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 1550 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9345, grad_fn=<SubBackward0>)
Episode: 1551 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 1552 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1553 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1554 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1555 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1556 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1557 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1558 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1559 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1560 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 1561 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 1562 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1563 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1564 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0837, grad_fn=<SubBackward0>)
Episode: 1565 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1566 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 1567 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7950, grad_fn=<SubBackward0>)
Episode: 1568 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1569 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 1570 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1571 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 1572 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1573 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 1574 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1431, grad_fn=<SubBackward0>)
Episode: 1575 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 1576 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1838, grad_fn=<SubBackward0>)
Episode: 1577 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1578 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 1579 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 1580 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1581 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1582 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1583 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1584 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 1585 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7904, grad_fn=<SubBackward0>)
Episode: 1586 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1587 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 1588 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1589 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 1590 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1591 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6878, grad_fn=<SubBackward0>)
Episode: 1592 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7730, grad_fn=<SubBackward0>)
Episode: 1593 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0841, grad_fn=<SubBackward0>)
Episode: 1594 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 1595 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1676, grad_fn=<SubBackward0>)
Episode: 1596 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1597 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1598 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1599 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1600 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1601 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1602 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 1603 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1604 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1605 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1606 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1607 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1608 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1609 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1610 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1611 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1612 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1613 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1614 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1615 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0668, grad_fn=<SubBackward0>)
Episode: 1616 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.8980, grad_fn=<SubBackward0>)
Episode: 1617 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 1618 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 1619 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0397, grad_fn=<SubBackward0>)
Episode: 1620 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 1621 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1622 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 1623 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1624 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1625 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1626 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 1627 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9250, grad_fn=<SubBackward0>)
Episode: 1628 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6193, grad_fn=<SubBackward0>)
Episode: 1629 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 1630 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 1631 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 1632 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 1633 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7493, grad_fn=<SubBackward0>)
Episode: 1634 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1635 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1636 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1637 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1241, grad_fn=<SubBackward0>)
Episode: 1638 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8274, grad_fn=<SubBackward0>)
Episode: 1639 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1640 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 1641 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 1642 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1210, grad_fn=<SubBackward0>)
Episode: 1643 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1200, grad_fn=<SubBackward0>)
Episode: 1644 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 1645 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1944, grad_fn=<SubBackward0>)
Episode: 1646 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1206, grad_fn=<SubBackward0>)
Episode: 1647 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 1648 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 1649 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 1650 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 1651 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 1652 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 1653 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 1654 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 1655 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 1656 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9170, grad_fn=<SubBackward0>)
Episode: 1657 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 1658 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 1659 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9831, grad_fn=<SubBackward0>)
Episode: 1660 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1661 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 1662 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1663 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7104, grad_fn=<SubBackward0>)
Episode: 1664 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 1665 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8383, grad_fn=<SubBackward0>)
Episode: 1666 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1667 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1668 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1669 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 1670 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 1671 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 1672 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1673 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 1674 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1675 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 1676 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1206, grad_fn=<SubBackward0>)
Episode: 1677 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1233, grad_fn=<SubBackward0>)
Episode: 1678 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9624, grad_fn=<SubBackward0>)
Episode: 1679 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 1680 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 1681 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 1682 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2903, grad_fn=<SubBackward0>)
Episode: 1683 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1684 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1685 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 1686 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9596, grad_fn=<SubBackward0>)
Episode: 1687 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9945, grad_fn=<SubBackward0>)
Episode: 1688 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 1689 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1210, grad_fn=<SubBackward0>)
Episode: 1690 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8649, grad_fn=<SubBackward0>)
Episode: 1691 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 1692 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 1693 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1694 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 1695 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 1696 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1228, grad_fn=<SubBackward0>)
Episode: 1697 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 1698 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 1699 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9893, grad_fn=<SubBackward0>)
Episode: 1700 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 1701 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1702 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8473, grad_fn=<SubBackward0>)
Episode: 1703 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9476, grad_fn=<SubBackward0>)
Episode: 1704 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 1705 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 1706 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 1707 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1208, grad_fn=<SubBackward0>)
Episode: 1708 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9048, grad_fn=<SubBackward0>)
Episode: 1709 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1237, grad_fn=<SubBackward0>)
Episode: 1710 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1212, grad_fn=<SubBackward0>)
Episode: 1711 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1179, grad_fn=<SubBackward0>)
Episode: 1712 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 1713 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1175, grad_fn=<SubBackward0>)
Episode: 1714 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1715 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1212, grad_fn=<SubBackward0>)
Episode: 1716 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1717 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1207, grad_fn=<SubBackward0>)
Episode: 1718 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1173, grad_fn=<SubBackward0>)
Episode: 1719 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1215, grad_fn=<SubBackward0>)
Episode: 1720 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0864, grad_fn=<SubBackward0>)
Episode: 1721 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1235, grad_fn=<SubBackward0>)
Episode: 1722 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1225, grad_fn=<SubBackward0>)
Episode: 1723 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1161, grad_fn=<SubBackward0>)
Episode: 1724 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1225, grad_fn=<SubBackward0>)
Episode: 1725 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1160, grad_fn=<SubBackward0>)
Episode: 1726 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9546, grad_fn=<SubBackward0>)
Episode: 1727 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 1728 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1238, grad_fn=<SubBackward0>)
Episode: 1729 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 1730 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.4288, grad_fn=<SubBackward0>)
Episode: 1731 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1196, grad_fn=<SubBackward0>)
Episode: 1732 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1228, grad_fn=<SubBackward0>)
Episode: 1733 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 1734 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 1735 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 1736 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 1737 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 1738 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9928, grad_fn=<SubBackward0>)
Episode: 1739 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9649, grad_fn=<SubBackward0>)
Episode: 1740 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 1741 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 1742 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 1743 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8608, grad_fn=<SubBackward0>)
Episode: 1744 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1229, grad_fn=<SubBackward0>)
Episode: 1745 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1231, grad_fn=<SubBackward0>)
Episode: 1746 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 1747 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8310, grad_fn=<SubBackward0>)
Episode: 1748 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 1749 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 1750 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 1751 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 1752 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2656, grad_fn=<SubBackward0>)
Episode: 1753 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 1754 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 1755 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8443, grad_fn=<SubBackward0>)
Episode: 1756 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 1757 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1238, grad_fn=<SubBackward0>)
Episode: 1758 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1225, grad_fn=<SubBackward0>)
Episode: 1759 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 1760 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6523, grad_fn=<SubBackward0>)
Episode: 1761 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1213, grad_fn=<SubBackward0>)
Episode: 1762 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1763 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 1764 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 1765 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8471, grad_fn=<SubBackward0>)
Episode: 1766 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1178, grad_fn=<SubBackward0>)
Episode: 1767 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1664, grad_fn=<SubBackward0>)
Episode: 1768 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1207, grad_fn=<SubBackward0>)
Episode: 1769 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 1770 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1194, grad_fn=<SubBackward0>)
Episode: 1771 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6404, grad_fn=<SubBackward0>)
Episode: 1772 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 1773 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 1774 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8189, grad_fn=<SubBackward0>)
Episode: 1775 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8522, grad_fn=<SubBackward0>)
Episode: 1776 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 1777 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 1778 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1188, grad_fn=<SubBackward0>)
Episode: 1779 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1219, grad_fn=<SubBackward0>)
Episode: 1780 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1218, grad_fn=<SubBackward0>)
Episode: 1781 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1175, grad_fn=<SubBackward0>)
Episode: 1782 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1226, grad_fn=<SubBackward0>)
Episode: 1783 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1210, grad_fn=<SubBackward0>)
Episode: 1784 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0843, grad_fn=<SubBackward0>)
Episode: 1785 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1212, grad_fn=<SubBackward0>)
Episode: 1786 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.4454, grad_fn=<SubBackward0>)
Episode: 1787 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1216, grad_fn=<SubBackward0>)
Episode: 1788 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1789 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1213, grad_fn=<SubBackward0>)
Episode: 1790 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1225, grad_fn=<SubBackward0>)
Episode: 1791 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 1792 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1237, grad_fn=<SubBackward0>)
Episode: 1793 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 1794 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1795 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9692, grad_fn=<SubBackward0>)
Episode: 1796 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 1797 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1798 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1212, grad_fn=<SubBackward0>)
Episode: 1799 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 1800 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0908, grad_fn=<SubBackward0>)
Episode: 1801 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 1802 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1229, grad_fn=<SubBackward0>)
Episode: 1803 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9489, grad_fn=<SubBackward0>)
Episode: 1804 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 1805 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 1806 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1807 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 1808 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1809 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1810 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 1811 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1812 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 1813 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1814 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1815 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 1816 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3300, grad_fn=<SubBackward0>)
Episode: 1817 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1818 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 1819 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.7805, grad_fn=<SubBackward0>)
Episode: 1820 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8963, grad_fn=<SubBackward0>)
Episode: 1821 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1822 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1936, grad_fn=<SubBackward0>)
Episode: 1823 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 1824 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1825 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2406, grad_fn=<SubBackward0>)
Episode: 1826 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 1827 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1828 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 1829 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1830 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 1831 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0364, grad_fn=<SubBackward0>)
Episode: 1832 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1243, grad_fn=<SubBackward0>)
Episode: 1833 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8625, grad_fn=<SubBackward0>)
Episode: 1834 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 1835 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1411, grad_fn=<SubBackward0>)
Episode: 1836 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.4850, grad_fn=<SubBackward0>)
Episode: 1837 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1236, grad_fn=<SubBackward0>)
Episode: 1838 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7692, grad_fn=<SubBackward0>)
Episode: 1839 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0638, grad_fn=<SubBackward0>)
Episode: 1840 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 1841 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 1842 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 1843 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9264, grad_fn=<SubBackward0>)
Episode: 1844 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 1845 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9105, grad_fn=<SubBackward0>)
Episode: 1846 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 1847 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1848 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1849 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1220, grad_fn=<SubBackward0>)
Episode: 1850 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 1851 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9566, grad_fn=<SubBackward0>)
Episode: 1852 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1853 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1854 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 1855 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1856 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 1857 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1213, grad_fn=<SubBackward0>)
Episode: 1858 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 1859 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1860 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8893, grad_fn=<SubBackward0>)
Episode: 1861 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0920, grad_fn=<SubBackward0>)
Episode: 1862 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 1863 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 1864 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 1865 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1866 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1867 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9969, grad_fn=<SubBackward0>)
Episode: 1868 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1869 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1870 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1871 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 1872 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 1873 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1874 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1875 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1876 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1877 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1878 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 1879 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 1880 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 1881 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1882 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 1883 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0644, grad_fn=<SubBackward0>)
Episode: 1884 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1885 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 1886 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 1887 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1697, grad_fn=<SubBackward0>)
Episode: 1888 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1889 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1890 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 1891 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1892 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 1893 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 1894 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1895 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1896 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 1897 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1898 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1899 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1900 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 1901 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1902 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 1903 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 1904 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0243, grad_fn=<SubBackward0>)
Episode: 1905 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1906 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1518, grad_fn=<SubBackward0>)
Episode: 1907 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 1908 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 1909 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1910 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1911 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.5789, grad_fn=<SubBackward0>)
Episode: 1912 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 1913 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 1914 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1915 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 1916 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1917 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1918 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1919 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 1920 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8096, grad_fn=<SubBackward0>)
Episode: 1921 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0464, grad_fn=<SubBackward0>)
Episode: 1922 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1923 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 1924 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 1925 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 1926 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 1927 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.5365, grad_fn=<SubBackward0>)
Episode: 1928 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1929 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 1930 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 1931 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 1932 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 1933 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 1934 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 1935 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 1936 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 1937 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.5906, grad_fn=<SubBackward0>)
Episode: 1938 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 1939 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 1940 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 1941 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 1942 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1943 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8769, grad_fn=<SubBackward0>)
Episode: 1944 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 1945 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 1946 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 1947 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9564, grad_fn=<SubBackward0>)
Episode: 1948 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 1949 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 1950 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1241, grad_fn=<SubBackward0>)
Episode: 1951 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9253, grad_fn=<SubBackward0>)
Episode: 1952 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1231, grad_fn=<SubBackward0>)
Episode: 1953 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1954 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 1955 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 1956 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 1957 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 1958 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 1959 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7021, grad_fn=<SubBackward0>)
Episode: 1960 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 1961 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1962 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 1963 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 1964 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9686, grad_fn=<SubBackward0>)
Episode: 1965 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 1966 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 1967 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 1968 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8334, grad_fn=<SubBackward0>)
Episode: 1969 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3209, grad_fn=<SubBackward0>)
Episode: 1970 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9424, grad_fn=<SubBackward0>)
Episode: 1971 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1972 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1973 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 1974 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 1975 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0333, grad_fn=<SubBackward0>)
Episode: 1976 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 1977 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1978 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 1979 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 1980 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 1981 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 1982 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 1983 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 1984 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 1985 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 1986 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 1987 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 1988 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 1989 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 1990 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 1991 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9614, grad_fn=<SubBackward0>)
Episode: 1992 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2754, grad_fn=<SubBackward0>)
Episode: 1993 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 1994 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 1995 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 1996 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 1997 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2949, grad_fn=<SubBackward0>)
Episode: 1998 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 1999 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 2000 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 2001 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 2002 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 2003 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 2004 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 2005 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 2006 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 2007 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2008 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6901, grad_fn=<SubBackward0>)
Episode: 2009 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2010 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2011 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2012 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9407, grad_fn=<SubBackward0>)
Episode: 2013 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 2014 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2015 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8676, grad_fn=<SubBackward0>)
Episode: 2016 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 2017 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 2018 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 2019 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9678, grad_fn=<SubBackward0>)
Episode: 2020 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 2021 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 2022 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 2023 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 2024 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2025 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 2026 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 2027 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 2028 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 2029 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 2030 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 2031 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9544, grad_fn=<SubBackward0>)
Episode: 2032 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 2033 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 2034 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 2035 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 2036 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2037 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 2038 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7966, grad_fn=<SubBackward0>)
Episode: 2039 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 2040 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 2041 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 2042 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 2043 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2044 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 2045 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 2046 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1232, grad_fn=<SubBackward0>)
Episode: 2047 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2173, grad_fn=<SubBackward0>)
Episode: 2048 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 2049 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 2050 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 2051 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 2052 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 2053 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 2054 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 2055 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 2056 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 2057 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2058 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1209, grad_fn=<SubBackward0>)
Episode: 2059 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9501, grad_fn=<SubBackward0>)
Episode: 2060 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1218, grad_fn=<SubBackward0>)
Episode: 2061 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2062 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1176, grad_fn=<SubBackward0>)
Episode: 2063 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1192, grad_fn=<SubBackward0>)
Episode: 2064 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 2065 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2066 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1231, grad_fn=<SubBackward0>)
Episode: 2067 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 2068 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 2069 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 2070 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7491, grad_fn=<SubBackward0>)
Episode: 2071 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 2072 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 2073 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 2074 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0954, grad_fn=<SubBackward0>)
Episode: 2075 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 2076 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 2077 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2078 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7534, grad_fn=<SubBackward0>)
Episode: 2079 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 2080 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1224, grad_fn=<SubBackward0>)
Episode: 2081 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 2082 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2755, grad_fn=<SubBackward0>)
Episode: 2083 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 2084 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 2085 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 2086 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.0862, grad_fn=<SubBackward0>)
Episode: 2087 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 2088 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7176, grad_fn=<SubBackward0>)
Episode: 2089 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7473, grad_fn=<SubBackward0>)
Episode: 2090 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.4493, grad_fn=<SubBackward0>)
Episode: 2091 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 2092 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 2093 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2094 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 2095 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 2096 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 2097 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2098 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2099 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 2100 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2101 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2102 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 2103 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 2104 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 2105 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7655, grad_fn=<SubBackward0>)
Episode: 2106 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 2107 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7109, grad_fn=<SubBackward0>)
Episode: 2108 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 2109 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 2110 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2111 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2112 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 2113 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 2114 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 2115 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 2116 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 2117 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 2118 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 2119 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2120 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 2121 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 2122 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7887, grad_fn=<SubBackward0>)
Episode: 2123 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3137, grad_fn=<SubBackward0>)
Episode: 2124 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8064, grad_fn=<SubBackward0>)
Episode: 2125 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 2126 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 2127 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.4127, grad_fn=<SubBackward0>)
Episode: 2128 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 2129 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 2130 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2276, grad_fn=<SubBackward0>)
Episode: 2131 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 2132 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 2133 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 2134 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2135 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0589, grad_fn=<SubBackward0>)
Episode: 2136 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 2137 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 2138 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 2139 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 2140 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 2141 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 2142 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 2143 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 2144 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 2145 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9880, grad_fn=<SubBackward0>)
Episode: 2146 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9161, grad_fn=<SubBackward0>)
Episode: 2147 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2148 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 2149 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 2150 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 2151 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0510, grad_fn=<SubBackward0>)
Episode: 2152 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9049, grad_fn=<SubBackward0>)
Episode: 2153 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 2154 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 2155 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2156 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7255, grad_fn=<SubBackward0>)
Episode: 2157 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 2158 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9138, grad_fn=<SubBackward0>)
Episode: 2159 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 2160 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7727, grad_fn=<SubBackward0>)
Episode: 2161 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 2162 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2163 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 2164 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 2165 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8505, grad_fn=<SubBackward0>)
Episode: 2166 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 2167 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 2168 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8689, grad_fn=<SubBackward0>)
Episode: 2169 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 2170 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9200, grad_fn=<SubBackward0>)
Episode: 2171 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2172 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0400, grad_fn=<SubBackward0>)
Episode: 2173 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1241, grad_fn=<SubBackward0>)
Episode: 2174 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6432, grad_fn=<SubBackward0>)
Episode: 2175 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 2176 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2177 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 2178 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2179 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2180 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 2181 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2182 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2183 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 2184 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 2185 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 2186 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 2187 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 2188 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7468, grad_fn=<SubBackward0>)
Episode: 2189 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 2190 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1532, grad_fn=<SubBackward0>)
Episode: 2191 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 2192 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 2193 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 2194 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 2195 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 2196 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 2197 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8072, grad_fn=<SubBackward0>)
Episode: 2198 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 2199 | Train Reward: tensor([1.]) | Train Loss: tensor(0.7951, grad_fn=<SubBackward0>)
Episode: 2200 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 2201 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9958, grad_fn=<SubBackward0>)
Episode: 2202 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8715, grad_fn=<SubBackward0>)
Episode: 2203 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1230, grad_fn=<SubBackward0>)
Episode: 2204 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 2205 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1223, grad_fn=<SubBackward0>)
Episode: 2206 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1191, grad_fn=<SubBackward0>)
Episode: 2207 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1199, grad_fn=<SubBackward0>)
Episode: 2208 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 2209 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.7208, grad_fn=<SubBackward0>)
Episode: 2210 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9747, grad_fn=<SubBackward0>)
Episode: 2211 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 2212 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.6823, grad_fn=<SubBackward0>)
Episode: 2213 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1229, grad_fn=<SubBackward0>)
Episode: 2214 | Train Reward: tensor([1.]) | Train Loss: tensor(0.7452, grad_fn=<SubBackward0>)
Episode: 2215 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1131, grad_fn=<SubBackward0>)
Episode: 2216 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1222, grad_fn=<SubBackward0>)
Episode: 2217 | Train Reward: tensor([1.]) | Train Loss: tensor(0.7396, grad_fn=<SubBackward0>)
Episode: 2218 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1378, grad_fn=<SubBackward0>)
Episode: 2219 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1110, grad_fn=<SubBackward0>)
Episode: 2220 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1144, grad_fn=<SubBackward0>)
Episode: 2221 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6613, grad_fn=<SubBackward0>)
Episode: 2222 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1130, grad_fn=<SubBackward0>)
Episode: 2223 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1173, grad_fn=<SubBackward0>)
Episode: 2224 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1084, grad_fn=<SubBackward0>)
Episode: 2225 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1116, grad_fn=<SubBackward0>)
Episode: 2226 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7896, grad_fn=<SubBackward0>)
Episode: 2227 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6811, grad_fn=<SubBackward0>)
Episode: 2228 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1092, grad_fn=<SubBackward0>)
Episode: 2229 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1164, grad_fn=<SubBackward0>)
Episode: 2230 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1169, grad_fn=<SubBackward0>)
Episode: 2231 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1118, grad_fn=<SubBackward0>)
Episode: 2232 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1142, grad_fn=<SubBackward0>)
Episode: 2233 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1086, grad_fn=<SubBackward0>)
Episode: 2234 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1089, grad_fn=<SubBackward0>)
Episode: 2235 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1134, grad_fn=<SubBackward0>)
Episode: 2236 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1102, grad_fn=<SubBackward0>)
Episode: 2237 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1178, grad_fn=<SubBackward0>)
Episode: 2238 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1149, grad_fn=<SubBackward0>)
Episode: 2239 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1177, grad_fn=<SubBackward0>)
Episode: 2240 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1794, grad_fn=<SubBackward0>)
Episode: 2241 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1057, grad_fn=<SubBackward0>)
Episode: 2242 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1170, grad_fn=<SubBackward0>)
Episode: 2243 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1158, grad_fn=<SubBackward0>)
Episode: 2244 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1135, grad_fn=<SubBackward0>)
Episode: 2245 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1131, grad_fn=<SubBackward0>)
Episode: 2246 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2631, grad_fn=<SubBackward0>)
Episode: 2247 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8554, grad_fn=<SubBackward0>)
Episode: 2248 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1178, grad_fn=<SubBackward0>)
Episode: 2249 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 2250 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0392, grad_fn=<SubBackward0>)
Episode: 2251 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1208, grad_fn=<SubBackward0>)
Episode: 2252 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1176, grad_fn=<SubBackward0>)
Episode: 2253 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1212, grad_fn=<SubBackward0>)
Episode: 2254 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1327, grad_fn=<SubBackward0>)
Episode: 2255 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 2256 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 2257 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2258 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6189, grad_fn=<SubBackward0>)
Episode: 2259 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 2260 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9975, grad_fn=<SubBackward0>)
Episode: 2261 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8279, grad_fn=<SubBackward0>)
Episode: 2262 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 2263 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 2264 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 2265 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9975, grad_fn=<SubBackward0>)
Episode: 2266 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0624, grad_fn=<SubBackward0>)
Episode: 2267 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1196, grad_fn=<SubBackward0>)
Episode: 2268 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8070, grad_fn=<SubBackward0>)
Episode: 2269 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 2270 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 2271 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 2272 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1343, grad_fn=<SubBackward0>)
Episode: 2273 | Train Reward: tensor([1.]) | Train Loss: tensor(0.7764, grad_fn=<SubBackward0>)
Episode: 2274 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1184, grad_fn=<SubBackward0>)
Episode: 2275 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 2276 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1241, grad_fn=<SubBackward0>)
Episode: 2277 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 2278 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1229, grad_fn=<SubBackward0>)
Episode: 2279 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1229, grad_fn=<SubBackward0>)
Episode: 2280 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 2281 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7761, grad_fn=<SubBackward0>)
Episode: 2282 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1175, grad_fn=<SubBackward0>)
Episode: 2283 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1219, grad_fn=<SubBackward0>)
Episode: 2284 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1194, grad_fn=<SubBackward0>)
Episode: 2285 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 2286 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1233, grad_fn=<SubBackward0>)
Episode: 2287 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1219, grad_fn=<SubBackward0>)
Episode: 2288 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 2289 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 2290 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 2291 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 2292 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 2293 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 2294 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1197, grad_fn=<SubBackward0>)
Episode: 2295 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 2296 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 2297 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 2298 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 2299 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8201, grad_fn=<SubBackward0>)
Episode: 2300 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 2301 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 2302 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7937, grad_fn=<SubBackward0>)
Episode: 2303 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 2304 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1320, grad_fn=<SubBackward0>)
Episode: 2305 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 2306 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 2307 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 2308 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 2309 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 2310 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 2311 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3282, grad_fn=<SubBackward0>)
Episode: 2312 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 2313 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 2314 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2315 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 2316 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2317 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 2318 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9919, grad_fn=<SubBackward0>)
Episode: 2319 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1341, grad_fn=<SubBackward0>)
Episode: 2320 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0778, grad_fn=<SubBackward0>)
Episode: 2321 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2322 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 2323 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2324 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0328, grad_fn=<SubBackward0>)
Episode: 2325 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 2326 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1093, grad_fn=<SubBackward0>)
Episode: 2327 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 2328 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 2329 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 2330 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 2331 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 2332 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 2333 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 2334 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 2335 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 2336 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9062, grad_fn=<SubBackward0>)
Episode: 2337 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 2338 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 2339 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8630, grad_fn=<SubBackward0>)
Episode: 2340 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 2341 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2342 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 2343 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 2344 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0520, grad_fn=<SubBackward0>)
Episode: 2345 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0604, grad_fn=<SubBackward0>)
Episode: 2346 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 2347 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 2348 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 2349 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 2350 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 2351 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 2352 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 2353 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 2354 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 2355 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 2356 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 2357 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2358 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8569, grad_fn=<SubBackward0>)
Episode: 2359 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2360 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 2361 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 2362 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 2363 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 2364 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 2365 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 2366 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7924, grad_fn=<SubBackward0>)
Episode: 2367 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 2368 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 2369 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 2370 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 2371 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1379, grad_fn=<SubBackward0>)
Episode: 2372 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2373 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 2374 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7890, grad_fn=<SubBackward0>)
Episode: 2375 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 2376 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 2377 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2378 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 2379 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 2380 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2381 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 2382 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 2383 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2384 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 2385 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2386 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2387 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 2388 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 2389 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 2390 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 2391 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 2392 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 2393 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 2394 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 2395 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1815, grad_fn=<SubBackward0>)
Episode: 2396 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 2397 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9751, grad_fn=<SubBackward0>)
Episode: 2398 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2399 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1034, grad_fn=<SubBackward0>)
Episode: 2400 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 2401 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2402 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 2403 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 2404 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 2405 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 2406 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 2407 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7294, grad_fn=<SubBackward0>)
Episode: 2408 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 2409 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 2410 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 2411 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 2412 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7342, grad_fn=<SubBackward0>)
Episode: 2413 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7735, grad_fn=<SubBackward0>)
Episode: 2414 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1749, grad_fn=<SubBackward0>)
Episode: 2415 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 2416 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2417 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 2418 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 2419 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1312, grad_fn=<SubBackward0>)
Episode: 2420 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 2421 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 2422 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0207, grad_fn=<SubBackward0>)
Episode: 2423 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7421, grad_fn=<SubBackward0>)
Episode: 2424 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2425 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 2426 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 2427 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 2428 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 2429 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 2430 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 2431 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 2432 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7687, grad_fn=<SubBackward0>)
Episode: 2433 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1744, grad_fn=<SubBackward0>)
Episode: 2434 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8220, grad_fn=<SubBackward0>)
Episode: 2435 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1118, grad_fn=<SubBackward0>)
Episode: 2436 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 2437 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 2438 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 2439 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 2440 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2441 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 2442 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1719, grad_fn=<SubBackward0>)
Episode: 2443 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 2444 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 2445 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2446 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 2447 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 2448 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 2449 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2450 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0506, grad_fn=<SubBackward0>)
Episode: 2451 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 2452 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2453 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 2454 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2455 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 2456 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 2457 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 2458 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8875, grad_fn=<SubBackward0>)
Episode: 2459 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 2460 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 2461 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 2462 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 2463 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 2464 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 2465 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 2466 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 2467 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 2468 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 2469 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 2470 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 2471 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 2472 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6864, grad_fn=<SubBackward0>)
Episode: 2473 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 2474 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8330, grad_fn=<SubBackward0>)
Episode: 2475 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 2476 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1567, grad_fn=<SubBackward0>)
Episode: 2477 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 2478 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1224, grad_fn=<SubBackward0>)
Episode: 2479 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 2480 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7471, grad_fn=<SubBackward0>)
Episode: 2481 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 2482 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 2483 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 2484 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 2485 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 2486 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0843, grad_fn=<SubBackward0>)
Episode: 2487 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 2488 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 2489 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 2490 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0279, grad_fn=<SubBackward0>)
Episode: 2491 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 2492 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 2493 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 2494 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 2495 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 2496 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 2497 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 2498 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7327, grad_fn=<SubBackward0>)
Episode: 2499 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 2500 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 2501 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8389, grad_fn=<SubBackward0>)
Episode: 2502 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2503 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 2504 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 2505 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2506 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 2507 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2508 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 2509 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 2510 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 2511 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 2512 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 2513 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 2514 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2515 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 2516 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 2517 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2125, grad_fn=<SubBackward0>)
Episode: 2518 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 2519 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 2520 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0488, grad_fn=<SubBackward0>)
Episode: 2521 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 2522 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1381, grad_fn=<SubBackward0>)
Episode: 2523 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 2524 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 2525 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 2526 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 2527 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 2528 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 2529 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 2530 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 2531 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9343, grad_fn=<SubBackward0>)
Episode: 2532 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 2533 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 2534 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2535 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3669, grad_fn=<SubBackward0>)
Episode: 2536 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1384, grad_fn=<SubBackward0>)
Episode: 2537 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2538 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2539 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8373, grad_fn=<SubBackward0>)
Episode: 2540 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 2541 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 2542 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 2543 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 2544 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1513, grad_fn=<SubBackward0>)
Episode: 2545 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 2546 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 2547 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 2548 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1383, grad_fn=<SubBackward0>)
Episode: 2549 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 2550 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 2551 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 2552 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 2553 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 2554 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2738, grad_fn=<SubBackward0>)
Episode: 2555 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 2556 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7301, grad_fn=<SubBackward0>)
Episode: 2557 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 2558 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 2559 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 2560 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9754, grad_fn=<SubBackward0>)
Episode: 2561 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 2562 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 2563 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 2564 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1223, grad_fn=<SubBackward0>)
Episode: 2565 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 2566 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0057, grad_fn=<SubBackward0>)
Episode: 2567 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 2568 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2287, grad_fn=<SubBackward0>)
Episode: 2569 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 2570 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3345, grad_fn=<SubBackward0>)
Episode: 2571 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 2572 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 2573 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2574 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 2575 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 2576 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 2577 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 2578 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 2579 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 2580 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 2581 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 2582 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 2583 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2584 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7511, grad_fn=<SubBackward0>)
Episode: 2585 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2251, grad_fn=<SubBackward0>)
Episode: 2586 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 2587 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 2588 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 2589 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 2590 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 2591 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2592 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 2593 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 2594 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 2595 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0442, grad_fn=<SubBackward0>)
Episode: 2596 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2597 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3001, grad_fn=<SubBackward0>)
Episode: 2598 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2599 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2600 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 2601 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 2602 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 2603 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9462, grad_fn=<SubBackward0>)
Episode: 2604 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 2605 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9447, grad_fn=<SubBackward0>)
Episode: 2606 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 2607 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 2608 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9466, grad_fn=<SubBackward0>)
Episode: 2609 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 2610 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2611 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 2612 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2613 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 2614 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1231, grad_fn=<SubBackward0>)
Episode: 2615 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 2616 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 2617 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 2618 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 2619 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 2620 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 2621 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3681, grad_fn=<SubBackward0>)
Episode: 2622 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 2623 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 2624 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 2625 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2626 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 2627 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0752, grad_fn=<SubBackward0>)
Episode: 2628 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 2629 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 2630 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 2631 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.4380, grad_fn=<SubBackward0>)
Episode: 2632 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 2633 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2634 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1053, grad_fn=<SubBackward0>)
Episode: 2635 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8460, grad_fn=<SubBackward0>)
Episode: 2636 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1219, grad_fn=<SubBackward0>)
Episode: 2637 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1216, grad_fn=<SubBackward0>)
Episode: 2638 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7955, grad_fn=<SubBackward0>)
Episode: 2639 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7159, grad_fn=<SubBackward0>)
Episode: 2640 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1236, grad_fn=<SubBackward0>)
Episode: 2641 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 2642 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 2643 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 2644 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1229, grad_fn=<SubBackward0>)
Episode: 2645 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7418, grad_fn=<SubBackward0>)
Episode: 2646 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 2647 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1642, grad_fn=<SubBackward0>)
Episode: 2648 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 2649 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 2650 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 2651 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9323, grad_fn=<SubBackward0>)
Episode: 2652 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9073, grad_fn=<SubBackward0>)
Episode: 2653 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 2654 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 2655 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1695, grad_fn=<SubBackward0>)
Episode: 2656 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 2657 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 2658 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 2659 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 2660 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 2661 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2662 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 2663 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 2664 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0396, grad_fn=<SubBackward0>)
Episode: 2665 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2666 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 2667 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2668 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1237, grad_fn=<SubBackward0>)
Episode: 2669 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 2670 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 2671 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 2672 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 2673 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 2674 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 2675 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8368, grad_fn=<SubBackward0>)
Episode: 2676 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 2677 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0315, grad_fn=<SubBackward0>)
Episode: 2678 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 2679 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 2680 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0299, grad_fn=<SubBackward0>)
Episode: 2681 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6166, grad_fn=<SubBackward0>)
Episode: 2682 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9548, grad_fn=<SubBackward0>)
Episode: 2683 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 2684 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1237, grad_fn=<SubBackward0>)
Episode: 2685 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 2686 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 2687 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 2688 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2689 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 2690 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0480, grad_fn=<SubBackward0>)
Episode: 2691 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0154, grad_fn=<SubBackward0>)
Episode: 2692 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1182, grad_fn=<SubBackward0>)
Episode: 2693 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8461, grad_fn=<SubBackward0>)
Episode: 2694 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 2695 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 2696 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0409, grad_fn=<SubBackward0>)
Episode: 2697 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 2698 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1784, grad_fn=<SubBackward0>)
Episode: 2699 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 2700 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1228, grad_fn=<SubBackward0>)
Episode: 2701 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2702 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0151, grad_fn=<SubBackward0>)
Episode: 2703 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 2704 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 2705 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9722, grad_fn=<SubBackward0>)
Episode: 2706 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 2707 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 2708 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 2709 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 2710 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6766, grad_fn=<SubBackward0>)
Episode: 2711 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 2712 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 2713 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 2714 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1221, grad_fn=<SubBackward0>)
Episode: 2715 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7807, grad_fn=<SubBackward0>)
Episode: 2716 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 2717 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 2718 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 2719 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9224, grad_fn=<SubBackward0>)
Episode: 2720 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 2721 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1348, grad_fn=<SubBackward0>)
Episode: 2722 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7246, grad_fn=<SubBackward0>)
Episode: 2723 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8825, grad_fn=<SubBackward0>)
Episode: 2724 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1226, grad_fn=<SubBackward0>)
Episode: 2725 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 2726 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 2727 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 2728 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 2729 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9275, grad_fn=<SubBackward0>)
Episode: 2730 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1186, grad_fn=<SubBackward0>)
Episode: 2731 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7781, grad_fn=<SubBackward0>)
Episode: 2732 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 2733 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 2734 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 2735 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 2736 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 2737 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 2738 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 2739 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2740 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 2741 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1329, grad_fn=<SubBackward0>)
Episode: 2742 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.7798, grad_fn=<SubBackward0>)
Episode: 2743 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 2744 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 2745 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.7156, grad_fn=<SubBackward0>)
Episode: 2746 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 2747 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1225, grad_fn=<SubBackward0>)
Episode: 2748 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 2749 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 2750 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 2751 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 2752 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1205, grad_fn=<SubBackward0>)
Episode: 2753 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1224, grad_fn=<SubBackward0>)
Episode: 2754 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1193, grad_fn=<SubBackward0>)
Episode: 2755 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1235, grad_fn=<SubBackward0>)
Episode: 2756 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 2757 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1374, grad_fn=<SubBackward0>)
Episode: 2758 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1207, grad_fn=<SubBackward0>)
Episode: 2759 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1186, grad_fn=<SubBackward0>)
Episode: 2760 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1201, grad_fn=<SubBackward0>)
Episode: 2761 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1124, grad_fn=<SubBackward0>)
Episode: 2762 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.7777, grad_fn=<SubBackward0>)
Episode: 2763 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 2764 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8782, grad_fn=<SubBackward0>)
Episode: 2765 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 2766 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9048, grad_fn=<SubBackward0>)
Episode: 2767 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1129, grad_fn=<SubBackward0>)
Episode: 2768 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1223, grad_fn=<SubBackward0>)
Episode: 2769 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1220, grad_fn=<SubBackward0>)
Episode: 2770 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 2771 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 2772 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 2773 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.6715, grad_fn=<SubBackward0>)
Episode: 2774 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1232, grad_fn=<SubBackward0>)
Episode: 2775 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2776 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2777 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 2778 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1201, grad_fn=<SubBackward0>)
Episode: 2779 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 2780 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 2781 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1545, grad_fn=<SubBackward0>)
Episode: 2782 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 2783 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 2784 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 2785 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2786 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1053, grad_fn=<SubBackward0>)
Episode: 2787 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 2788 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0741, grad_fn=<SubBackward0>)
Episode: 2789 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 2790 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 2791 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 2792 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2793 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 2794 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 2795 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2796 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8247, grad_fn=<SubBackward0>)
Episode: 2797 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2798 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 2799 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 2800 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 2801 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2802 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 2803 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 2804 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 2805 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3449, grad_fn=<SubBackward0>)
Episode: 2806 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1216, grad_fn=<SubBackward0>)
Episode: 2807 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 2808 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8499, grad_fn=<SubBackward0>)
Episode: 2809 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 2810 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 2811 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 2812 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2813 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 2814 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9216, grad_fn=<SubBackward0>)
Episode: 2815 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 2816 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 2817 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 2818 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3939, grad_fn=<SubBackward0>)
Episode: 2819 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 2820 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2821 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7647, grad_fn=<SubBackward0>)
Episode: 2822 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 2823 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 2824 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 2825 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 2826 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 2827 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 2828 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 2829 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0259, grad_fn=<SubBackward0>)
Episode: 2830 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 2831 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 2832 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 2833 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 2834 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 2835 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7390, grad_fn=<SubBackward0>)
Episode: 2836 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 2837 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 2838 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 2839 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 2840 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8511, grad_fn=<SubBackward0>)
Episode: 2841 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 2842 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 2843 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2844 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2845 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9966, grad_fn=<SubBackward0>)
Episode: 2846 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2847 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7200, grad_fn=<SubBackward0>)
Episode: 2848 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2849 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0256, grad_fn=<SubBackward0>)
Episode: 2850 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7587, grad_fn=<SubBackward0>)
Episode: 2851 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 2852 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 2853 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 2854 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 2855 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 2856 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1277, grad_fn=<SubBackward0>)
Episode: 2857 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 2858 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 2859 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 2860 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 2861 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 2862 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 2863 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 2864 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2865 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 2866 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8566, grad_fn=<SubBackward0>)
Episode: 2867 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 2868 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2869 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 2870 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2871 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0033, grad_fn=<SubBackward0>)
Episode: 2872 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 2873 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 2874 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 2875 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 2876 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 2877 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 2878 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 2879 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 2880 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 2881 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 2882 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 2883 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 2884 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2885 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2886 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8764, grad_fn=<SubBackward0>)
Episode: 2887 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 2888 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 2889 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 2890 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 2891 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 2892 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2893 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9328, grad_fn=<SubBackward0>)
Episode: 2894 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7633, grad_fn=<SubBackward0>)
Episode: 2895 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 2896 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 2897 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 2898 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 2899 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3192, grad_fn=<SubBackward0>)
Episode: 2900 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2901 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8849, grad_fn=<SubBackward0>)
Episode: 2902 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8863, grad_fn=<SubBackward0>)
Episode: 2903 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 2904 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 2905 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 2906 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 2907 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 2908 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2909 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 2910 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 2911 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9703, grad_fn=<SubBackward0>)
Episode: 2912 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 2913 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 2914 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0328, grad_fn=<SubBackward0>)
Episode: 2915 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1366, grad_fn=<SubBackward0>)
Episode: 2916 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 2917 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 2918 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2919 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1137, grad_fn=<SubBackward0>)
Episode: 2920 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 2921 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2922 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 2923 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 2924 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 2925 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 2926 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 2927 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 2928 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2929 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1363, grad_fn=<SubBackward0>)
Episode: 2930 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 2931 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3485, grad_fn=<SubBackward0>)
Episode: 2932 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 2933 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 2934 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 2935 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 2936 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 2937 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 2938 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9706, grad_fn=<SubBackward0>)
Episode: 2939 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 2940 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 2941 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 2942 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2943 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 2944 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 2945 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 2946 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1350, grad_fn=<SubBackward0>)
Episode: 2947 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 2948 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 2949 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 2950 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 2951 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 2952 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1377, grad_fn=<SubBackward0>)
Episode: 2953 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 2954 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 2955 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 2956 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 2957 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0862, grad_fn=<SubBackward0>)
Episode: 2958 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 2959 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 2960 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 2961 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 2962 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1375, grad_fn=<SubBackward0>)
Episode: 2963 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9240, grad_fn=<SubBackward0>)
Episode: 2964 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 2965 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8442, grad_fn=<SubBackward0>)
Episode: 2966 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8677, grad_fn=<SubBackward0>)
Episode: 2967 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 2968 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 2969 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 2970 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 2971 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 2972 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1369, grad_fn=<SubBackward0>)
Episode: 2973 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 2974 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 2975 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 2976 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9197, grad_fn=<SubBackward0>)
Episode: 2977 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 2978 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 2979 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 2980 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2981 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1376, grad_fn=<SubBackward0>)
Episode: 2982 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.2530, grad_fn=<SubBackward0>)
Episode: 2983 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 2984 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 2985 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1382, grad_fn=<SubBackward0>)
Episode: 2986 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0051, grad_fn=<SubBackward0>)
Episode: 2987 | Train Reward: tensor([1.]) | Train Loss: tensor(1.4071, grad_fn=<SubBackward0>)
Episode: 2988 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 2989 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1373, grad_fn=<SubBackward0>)
Episode: 2990 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3216, grad_fn=<SubBackward0>)
Episode: 2991 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1371, grad_fn=<SubBackward0>)
Episode: 2992 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 2993 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 2994 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 2995 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1378, grad_fn=<SubBackward0>)
Episode: 2996 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 2997 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1370, grad_fn=<SubBackward0>)
Episode: 2998 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9341, grad_fn=<SubBackward0>)
Episode: 2999 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1354, grad_fn=<SubBackward0>)
Episode: 3000 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1357, grad_fn=<SubBackward0>)
Episode: 3001 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9695, grad_fn=<SubBackward0>)
Episode: 3002 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8443, grad_fn=<SubBackward0>)
Episode: 3003 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 3004 | Train Reward: tensor([1.]) | Train Loss: tensor(1.2819, grad_fn=<SubBackward0>)
Episode: 3005 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 3006 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 3007 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 3008 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1368, grad_fn=<SubBackward0>)
Episode: 3009 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 3010 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 3011 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 3012 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9272, grad_fn=<SubBackward0>)
Episode: 3013 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1374, grad_fn=<SubBackward0>)
Episode: 3014 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 3015 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7731, grad_fn=<SubBackward0>)
Episode: 3016 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 3017 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 3018 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0603, grad_fn=<SubBackward0>)
Episode: 3019 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3020 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 3021 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1372, grad_fn=<SubBackward0>)
Episode: 3022 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0901, grad_fn=<SubBackward0>)
Episode: 3023 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 3024 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3025 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8393, grad_fn=<SubBackward0>)
Episode: 3026 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7222, grad_fn=<SubBackward0>)
Episode: 3027 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0686, grad_fn=<SubBackward0>)
Episode: 3028 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9146, grad_fn=<SubBackward0>)
Episode: 3029 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 3030 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 3031 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 3032 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 3033 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 3034 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1367, grad_fn=<SubBackward0>)
Episode: 3035 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9535, grad_fn=<SubBackward0>)
Episode: 3036 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 3037 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3038 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1899, grad_fn=<SubBackward0>)
Episode: 3039 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3040 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 3041 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 3042 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6401, grad_fn=<SubBackward0>)
Episode: 3043 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 3044 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6788, grad_fn=<SubBackward0>)
Episode: 3045 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9469, grad_fn=<SubBackward0>)
Episode: 3046 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 3047 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 3048 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 3049 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 3050 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 3051 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 3052 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9129, grad_fn=<SubBackward0>)
Episode: 3053 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 3054 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 3055 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3056 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 3057 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3058 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9466, grad_fn=<SubBackward0>)
Episode: 3059 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 3060 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 3061 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9062, grad_fn=<SubBackward0>)
Episode: 3062 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1221, grad_fn=<SubBackward0>)
Episode: 3063 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2046, grad_fn=<SubBackward0>)
Episode: 3064 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1146, grad_fn=<SubBackward0>)
Episode: 3065 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 3066 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1222, grad_fn=<SubBackward0>)
Episode: 3067 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7648, grad_fn=<SubBackward0>)
Episode: 3068 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3069 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0217, grad_fn=<SubBackward0>)
Episode: 3070 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3071 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3072 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 3073 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 3074 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 3075 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 3076 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8875, grad_fn=<SubBackward0>)
Episode: 3077 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3078 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1197, grad_fn=<SubBackward0>)
Episode: 3079 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1211, grad_fn=<SubBackward0>)
Episode: 3080 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 3081 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 3082 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 3083 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3084 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 3085 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 3086 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1221, grad_fn=<SubBackward0>)
Episode: 3087 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 3088 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 3089 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 3090 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8990, grad_fn=<SubBackward0>)
Episode: 3091 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1201, grad_fn=<SubBackward0>)
Episode: 3092 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9739, grad_fn=<SubBackward0>)
Episode: 3093 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 3094 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1228, grad_fn=<SubBackward0>)
Episode: 3095 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1212, grad_fn=<SubBackward0>)
Episode: 3096 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6976, grad_fn=<SubBackward0>)
Episode: 3097 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3098 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 3099 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 3100 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 3101 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1131, grad_fn=<SubBackward0>)
Episode: 3102 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9835, grad_fn=<SubBackward0>)
Episode: 3103 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3237, grad_fn=<SubBackward0>)
Episode: 3104 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3105 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 3106 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8858, grad_fn=<SubBackward0>)
Episode: 3107 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 3108 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 3109 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 3110 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 3111 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 3112 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3113 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3114 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 3115 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7881, grad_fn=<SubBackward0>)
Episode: 3116 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3117 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1205, grad_fn=<SubBackward0>)
Episode: 3118 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3119 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1960, grad_fn=<SubBackward0>)
Episode: 3120 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 3121 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 3122 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3123 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1228, grad_fn=<SubBackward0>)
Episode: 3124 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 3125 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 3126 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8048, grad_fn=<SubBackward0>)
Episode: 3127 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 3128 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1200, grad_fn=<SubBackward0>)
Episode: 3129 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 3130 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 3131 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1251, grad_fn=<SubBackward0>)
Episode: 3132 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0382, grad_fn=<SubBackward0>)
Episode: 3133 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 3134 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3135 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1781, grad_fn=<SubBackward0>)
Episode: 3136 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 3137 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 3138 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 3139 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 3140 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6686, grad_fn=<SubBackward0>)
Episode: 3141 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 3142 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 3143 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 3144 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0201, grad_fn=<SubBackward0>)
Episode: 3145 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3146 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 3147 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 3148 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1255, grad_fn=<SubBackward0>)
Episode: 3149 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 3150 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.2012, grad_fn=<SubBackward0>)
Episode: 3151 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 3152 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 3153 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3154 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 3155 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 3156 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3157 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 3158 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 3159 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3160 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9388, grad_fn=<SubBackward0>)
Episode: 3161 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 3162 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7946, grad_fn=<SubBackward0>)
Episode: 3163 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0819, grad_fn=<SubBackward0>)
Episode: 3164 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 3165 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 3166 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9885, grad_fn=<SubBackward0>)
Episode: 3167 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 3168 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 3169 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3170 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 3171 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 3172 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 3173 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 3174 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 3175 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 3176 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 3177 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3178 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9395, grad_fn=<SubBackward0>)
Episode: 3179 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 3180 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 3181 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 3182 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 3183 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 3184 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8610, grad_fn=<SubBackward0>)
Episode: 3185 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0030, grad_fn=<SubBackward0>)
Episode: 3186 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 3187 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 3188 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 3189 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 3190 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9694, grad_fn=<SubBackward0>)
Episode: 3191 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3192 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 3193 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6740, grad_fn=<SubBackward0>)
Episode: 3194 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 3195 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 3196 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 3197 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 3198 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 3199 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9603, grad_fn=<SubBackward0>)
Episode: 3200 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 3201 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 3202 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3203 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 3204 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 3205 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 3206 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 3207 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 3208 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 3209 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3210 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 3211 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 3212 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 3213 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3214 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 3215 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 3216 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 3217 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3218 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3219 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 3220 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1426, grad_fn=<SubBackward0>)
Episode: 3221 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1897, grad_fn=<SubBackward0>)
Episode: 3222 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 3223 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7226, grad_fn=<SubBackward0>)
Episode: 3224 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3225 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 3226 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 3227 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 3228 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3229 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7913, grad_fn=<SubBackward0>)
Episode: 3230 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 3231 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 3232 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 3233 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0575, grad_fn=<SubBackward0>)
Episode: 3234 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 3235 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 3236 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 3237 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 3238 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 3239 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8882, grad_fn=<SubBackward0>)
Episode: 3240 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 3241 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 3242 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 3243 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 3244 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 3245 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3246 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 3247 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 3248 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9663, grad_fn=<SubBackward0>)
Episode: 3249 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 3250 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8412, grad_fn=<SubBackward0>)
Episode: 3251 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0199, grad_fn=<SubBackward0>)
Episode: 3252 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 3253 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 3254 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9838, grad_fn=<SubBackward0>)
Episode: 3255 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 3256 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 3257 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 3258 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3259 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8624, grad_fn=<SubBackward0>)
Episode: 3260 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 3261 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3262 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8759, grad_fn=<SubBackward0>)
Episode: 3263 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 3264 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 3265 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.4003, grad_fn=<SubBackward0>)
Episode: 3266 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1274, grad_fn=<SubBackward0>)
Episode: 3267 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 3268 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9457, grad_fn=<SubBackward0>)
Episode: 3269 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1353, grad_fn=<SubBackward0>)
Episode: 3270 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3271 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 3272 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 3273 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 3274 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 3275 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 3276 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 3277 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 3278 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 3279 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 3280 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3281 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 3282 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 3283 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 3284 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 3285 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 3286 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 3287 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 3288 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 3289 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7378, grad_fn=<SubBackward0>)
Episode: 3290 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7520, grad_fn=<SubBackward0>)
Episode: 3291 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6911, grad_fn=<SubBackward0>)
Episode: 3292 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 3293 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 3294 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 3295 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 3296 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 3297 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 3298 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1360, grad_fn=<SubBackward0>)
Episode: 3299 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1365, grad_fn=<SubBackward0>)
Episode: 3300 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 3301 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 3302 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 3303 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8390, grad_fn=<SubBackward0>)
Episode: 3304 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 3305 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1361, grad_fn=<SubBackward0>)
Episode: 3306 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 3307 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8762, grad_fn=<SubBackward0>)
Episode: 3308 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9022, grad_fn=<SubBackward0>)
Episode: 3309 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 3310 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 3311 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 3312 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1358, grad_fn=<SubBackward0>)
Episode: 3313 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9713, grad_fn=<SubBackward0>)
Episode: 3314 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1349, grad_fn=<SubBackward0>)
Episode: 3315 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8447, grad_fn=<SubBackward0>)
Episode: 3316 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1348, grad_fn=<SubBackward0>)
Episode: 3317 | Train Reward: tensor([1.]) | Train Loss: tensor(1.3049, grad_fn=<SubBackward0>)
Episode: 3318 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 3319 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 3320 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 3321 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1673, grad_fn=<SubBackward0>)
Episode: 3322 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0298, grad_fn=<SubBackward0>)
Episode: 3323 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 3324 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1335, grad_fn=<SubBackward0>)
Episode: 3325 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 3326 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1798, grad_fn=<SubBackward0>)
Episode: 3327 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8923, grad_fn=<SubBackward0>)
Episode: 3328 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9650, grad_fn=<SubBackward0>)
Episode: 3329 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 3330 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 3331 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1347, grad_fn=<SubBackward0>)
Episode: 3332 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1362, grad_fn=<SubBackward0>)
Episode: 3333 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 3334 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 3335 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1355, grad_fn=<SubBackward0>)
Episode: 3336 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 3337 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1341, grad_fn=<SubBackward0>)
Episode: 3338 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 3339 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3340 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 3341 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 3342 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 3343 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 3344 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 3345 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1637, grad_fn=<SubBackward0>)
Episode: 3346 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7834, grad_fn=<SubBackward0>)
Episode: 3347 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 3348 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9610, grad_fn=<SubBackward0>)
Episode: 3349 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 3350 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 3351 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3352 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 3353 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 3354 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3355 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2243, grad_fn=<SubBackward0>)
Episode: 3356 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 3357 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 3358 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3359 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 3360 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 3361 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3362 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3363 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7795, grad_fn=<SubBackward0>)
Episode: 3364 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 3365 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 3366 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 3367 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7675, grad_fn=<SubBackward0>)
Episode: 3368 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 3369 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 3370 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 3371 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 3372 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7120, grad_fn=<SubBackward0>)
Episode: 3373 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1356, grad_fn=<SubBackward0>)
Episode: 3374 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3375 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6718, grad_fn=<SubBackward0>)
Episode: 3376 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 3377 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 3378 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 3379 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 3380 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 3381 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 3382 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 3383 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9179, grad_fn=<SubBackward0>)
Episode: 3384 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0271, grad_fn=<SubBackward0>)
Episode: 3385 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 3386 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 3387 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1671, grad_fn=<SubBackward0>)
Episode: 3388 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 3389 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3390 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1334, grad_fn=<SubBackward0>)
Episode: 3391 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3392 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9497, grad_fn=<SubBackward0>)
Episode: 3393 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8318, grad_fn=<SubBackward0>)
Episode: 3394 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 3395 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8967, grad_fn=<SubBackward0>)
Episode: 3396 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3397 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 3398 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9140, grad_fn=<SubBackward0>)
Episode: 3399 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 3400 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 3401 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1052, grad_fn=<SubBackward0>)
Episode: 3402 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8405, grad_fn=<SubBackward0>)
Episode: 3403 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 3404 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9979, grad_fn=<SubBackward0>)
Episode: 3405 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1321, grad_fn=<SubBackward0>)
Episode: 3406 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 3407 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1254, grad_fn=<SubBackward0>)
Episode: 3408 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3409 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3410 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 3411 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1238, grad_fn=<SubBackward0>)
Episode: 3412 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3413 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3414 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1226, grad_fn=<SubBackward0>)
Episode: 3415 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 3416 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 3417 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 3418 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 3419 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 3420 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1242, grad_fn=<SubBackward0>)
Episode: 3421 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 3422 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 3423 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 3424 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1338, grad_fn=<SubBackward0>)
Episode: 3425 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6685, grad_fn=<SubBackward0>)
Episode: 3426 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 3427 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 3428 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9837, grad_fn=<SubBackward0>)
Episode: 3429 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2298, grad_fn=<SubBackward0>)
Episode: 3430 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7308, grad_fn=<SubBackward0>)
Episode: 3431 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1237, grad_fn=<SubBackward0>)
Episode: 3432 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 3433 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1231, grad_fn=<SubBackward0>)
Episode: 3434 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1217, grad_fn=<SubBackward0>)
Episode: 3435 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6710, grad_fn=<SubBackward0>)
Episode: 3436 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3437 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 3438 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 3439 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 3440 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1237, grad_fn=<SubBackward0>)
Episode: 3441 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1935, grad_fn=<SubBackward0>)
Episode: 3442 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 3443 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 3444 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 3445 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9391, grad_fn=<SubBackward0>)
Episode: 3446 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 3447 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1288, grad_fn=<SubBackward0>)
Episode: 3448 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1316, grad_fn=<SubBackward0>)
Episode: 3449 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1252, grad_fn=<SubBackward0>)
Episode: 3450 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3451 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1209, grad_fn=<SubBackward0>)
Episode: 3452 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 3453 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 3454 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 3455 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 3456 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8512, grad_fn=<SubBackward0>)
Episode: 3457 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 3458 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3459 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7663, grad_fn=<SubBackward0>)
Episode: 3460 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1604, grad_fn=<SubBackward0>)
Episode: 3461 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3462 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
Episode: 3463 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 3464 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 3465 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 3466 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1285, grad_fn=<SubBackward0>)
Episode: 3467 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3468 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 3469 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7927, grad_fn=<SubBackward0>)
Episode: 3470 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 3471 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0905, grad_fn=<SubBackward0>)
Episode: 3472 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1695, grad_fn=<SubBackward0>)
Episode: 3473 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 3474 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 3475 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3476 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 3477 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 3478 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 3479 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 3480 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3481 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 3482 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 3483 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 3484 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 3485 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 3486 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3487 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 3488 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 3489 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 3490 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1130, grad_fn=<SubBackward0>)
Episode: 3491 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 3492 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1352, grad_fn=<SubBackward0>)
Episode: 3493 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 3494 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3495 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 3496 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8382, grad_fn=<SubBackward0>)
Episode: 3497 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 3498 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1228, grad_fn=<SubBackward0>)
Episode: 3499 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 3500 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8641, grad_fn=<SubBackward0>)
Episode: 3501 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1315, grad_fn=<SubBackward0>)
Episode: 3502 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 3503 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.1431, grad_fn=<SubBackward0>)
Episode: 3504 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3505 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9526, grad_fn=<SubBackward0>)
Episode: 3506 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 3507 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3508 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.9814, grad_fn=<SubBackward0>)
Episode: 3509 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 3510 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3511 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 3512 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8505, grad_fn=<SubBackward0>)
Episode: 3513 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1306, grad_fn=<SubBackward0>)
Episode: 3514 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 3515 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1337, grad_fn=<SubBackward0>)
Episode: 3516 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 3517 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3518 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0352, grad_fn=<SubBackward0>)
Episode: 3519 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 3520 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1259, grad_fn=<SubBackward0>)
Episode: 3521 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.8303, grad_fn=<SubBackward0>)
Episode: 3522 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 3523 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3524 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 3525 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1240, grad_fn=<SubBackward0>)
Episode: 3526 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 3527 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3528 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 3529 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6863, grad_fn=<SubBackward0>)
Episode: 3530 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3531 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1244, grad_fn=<SubBackward0>)
Episode: 3532 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3533 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9433, grad_fn=<SubBackward0>)
Episode: 3534 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 3535 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 3536 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7352, grad_fn=<SubBackward0>)
Episode: 3537 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1235, grad_fn=<SubBackward0>)
Episode: 3538 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 3539 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 3540 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 3541 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 3542 | Train Reward: tensor([1.]) | Train Loss: tensor(0.8726, grad_fn=<SubBackward0>)
Episode: 3543 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3544 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 3545 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0822, grad_fn=<SubBackward0>)
Episode: 3546 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9181, grad_fn=<SubBackward0>)
Episode: 3547 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 3548 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.4480, grad_fn=<SubBackward0>)
Episode: 3549 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 3550 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6266, grad_fn=<SubBackward0>)
Episode: 3551 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 3552 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1236, grad_fn=<SubBackward0>)
Episode: 3553 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 3554 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3555 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1234, grad_fn=<SubBackward0>)
Episode: 3556 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 3557 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 3558 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3559 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 3560 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 3561 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 3562 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 3563 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 3564 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1238, grad_fn=<SubBackward0>)
Episode: 3565 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 3566 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3567 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1257, grad_fn=<SubBackward0>)
Episode: 3568 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7293, grad_fn=<SubBackward0>)
Episode: 3569 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 3570 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3571 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 3572 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 3573 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3574 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9314, grad_fn=<SubBackward0>)
Episode: 3575 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 3576 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3577 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 3578 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3579 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9490, grad_fn=<SubBackward0>)
Episode: 3580 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3581 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1220, grad_fn=<SubBackward0>)
Episode: 3582 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 3583 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1228, grad_fn=<SubBackward0>)
Episode: 3584 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3585 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1228, grad_fn=<SubBackward0>)
Episode: 3586 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1239, grad_fn=<SubBackward0>)
Episode: 3587 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 3588 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 3589 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 3590 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 3591 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 3592 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1253, grad_fn=<SubBackward0>)
Episode: 3593 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 3594 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3595 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3596 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1249, grad_fn=<SubBackward0>)
Episode: 3597 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1222, grad_fn=<SubBackward0>)
Episode: 3598 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3599 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0624, grad_fn=<SubBackward0>)
Episode: 3600 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3601 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3602 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3603 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 3604 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9551, grad_fn=<SubBackward0>)
Episode: 3605 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 3606 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3607 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 3608 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 3609 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1283, grad_fn=<SubBackward0>)
Episode: 3610 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1278, grad_fn=<SubBackward0>)
Episode: 3611 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1266, grad_fn=<SubBackward0>)
Episode: 3612 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 3613 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6962, grad_fn=<SubBackward0>)
Episode: 3614 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 3615 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 3616 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8311, grad_fn=<SubBackward0>)
Episode: 3617 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1261, grad_fn=<SubBackward0>)
Episode: 3618 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3619 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 3620 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 3621 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3622 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 3623 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 3624 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 3625 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 3626 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 3627 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3628 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 3629 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1290, grad_fn=<SubBackward0>)
Episode: 3630 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9128, grad_fn=<SubBackward0>)
Episode: 3631 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1310, grad_fn=<SubBackward0>)
Episode: 3632 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 3633 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 3634 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1319, grad_fn=<SubBackward0>)
Episode: 3635 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 3636 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.3039, grad_fn=<SubBackward0>)
Episode: 3637 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1327, grad_fn=<SubBackward0>)
Episode: 3638 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1312, grad_fn=<SubBackward0>)
Episode: 3639 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 3640 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 3641 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 3642 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0965, grad_fn=<SubBackward0>)
Episode: 3643 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 3644 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6974, grad_fn=<SubBackward0>)
Episode: 3645 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 3646 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1305, grad_fn=<SubBackward0>)
Episode: 3647 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 3648 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 3649 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1331, grad_fn=<SubBackward0>)
Episode: 3650 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 3651 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8523, grad_fn=<SubBackward0>)
Episode: 3652 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9662, grad_fn=<SubBackward0>)
Episode: 3653 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 3654 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3655 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 3656 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 3657 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3658 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 3659 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 3660 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.1073, grad_fn=<SubBackward0>)
Episode: 3661 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 3662 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(0.8195, grad_fn=<SubBackward0>)
Episode: 3663 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3664 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3665 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3666 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1287, grad_fn=<SubBackward0>)
Episode: 3667 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1286, grad_fn=<SubBackward0>)
Episode: 3668 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.1727, grad_fn=<SubBackward0>)
Episode: 3669 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6848, grad_fn=<SubBackward0>)
Episode: 3670 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 3671 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 3672 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3673 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0711, grad_fn=<SubBackward0>)
Episode: 3674 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 3675 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 3676 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 3677 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1342, grad_fn=<SubBackward0>)
Episode: 3678 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1299, grad_fn=<SubBackward0>)
Episode: 3679 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1269, grad_fn=<SubBackward0>)
Episode: 3680 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3681 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1330, grad_fn=<SubBackward0>)
Episode: 3682 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 3683 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6685, grad_fn=<SubBackward0>)
Episode: 3684 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1248, grad_fn=<SubBackward0>)
Episode: 3685 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7036, grad_fn=<SubBackward0>)
Episode: 3686 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1246, grad_fn=<SubBackward0>)
Episode: 3687 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3688 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1247, grad_fn=<SubBackward0>)
Episode: 3689 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 3690 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 3691 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 3692 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1267, grad_fn=<SubBackward0>)
Episode: 3693 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 3694 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 3695 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 3696 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8528, grad_fn=<SubBackward0>)
Episode: 3697 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 3698 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0187, grad_fn=<SubBackward0>)
Episode: 3699 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1231, grad_fn=<SubBackward0>)
Episode: 3700 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1268, grad_fn=<SubBackward0>)
Episode: 3701 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 3702 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7877, grad_fn=<SubBackward0>)
Episode: 3703 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1281, grad_fn=<SubBackward0>)
Episode: 3704 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.1010, grad_fn=<SubBackward0>)
Episode: 3705 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3706 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1208, grad_fn=<SubBackward0>)
Episode: 3707 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3708 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 3709 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1277, grad_fn=<SubBackward0>)
Episode: 3710 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1210, grad_fn=<SubBackward0>)
Episode: 3711 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 3712 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1256, grad_fn=<SubBackward0>)
Episode: 3713 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1263, grad_fn=<SubBackward0>)
Episode: 3714 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.3403, grad_fn=<SubBackward0>)
Episode: 3715 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 3716 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8589, grad_fn=<SubBackward0>)
Episode: 3717 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1289, grad_fn=<SubBackward0>)
Episode: 3718 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 3719 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6977, grad_fn=<SubBackward0>)
Episode: 3720 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 3721 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9265, grad_fn=<SubBackward0>)
Episode: 3722 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1343, grad_fn=<SubBackward0>)
Episode: 3723 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 3724 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8424, grad_fn=<SubBackward0>)
Episode: 3725 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1339, grad_fn=<SubBackward0>)
Episode: 3726 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1300, grad_fn=<SubBackward0>)
Episode: 3727 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1336, grad_fn=<SubBackward0>)
Episode: 3728 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 3729 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6161, grad_fn=<SubBackward0>)
Episode: 3730 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 3731 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0085, grad_fn=<SubBackward0>)
Episode: 3732 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 3733 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 3734 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1344, grad_fn=<SubBackward0>)
Episode: 3735 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1280, grad_fn=<SubBackward0>)
Episode: 3736 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1333, grad_fn=<SubBackward0>)
Episode: 3737 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1324, grad_fn=<SubBackward0>)
Episode: 3738 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9319, grad_fn=<SubBackward0>)
Episode: 3739 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1328, grad_fn=<SubBackward0>)
Episode: 3740 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1332, grad_fn=<SubBackward0>)
Episode: 3741 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9883, grad_fn=<SubBackward0>)
Episode: 3742 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3743 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(1.0753, grad_fn=<SubBackward0>)
Episode: 3744 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1322, grad_fn=<SubBackward0>)
Episode: 3745 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 3746 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1293, grad_fn=<SubBackward0>)
Episode: 3747 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7169, grad_fn=<SubBackward0>)
Episode: 3748 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1229, grad_fn=<SubBackward0>)
Episode: 3749 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1273, grad_fn=<SubBackward0>)
Episode: 3750 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1314, grad_fn=<SubBackward0>)
Episode: 3751 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1323, grad_fn=<SubBackward0>)
Episode: 3752 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1224, grad_fn=<SubBackward0>)
Episode: 3753 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1282, grad_fn=<SubBackward0>)
Episode: 3754 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1275, grad_fn=<SubBackward0>)
Episode: 3755 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3756 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8737, grad_fn=<SubBackward0>)
Episode: 3757 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1284, grad_fn=<SubBackward0>)
Episode: 3758 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.6297, grad_fn=<SubBackward0>)
Episode: 3759 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3760 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 3761 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1262, grad_fn=<SubBackward0>)
Episode: 3762 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3763 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0322, grad_fn=<SubBackward0>)
Episode: 3764 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1297, grad_fn=<SubBackward0>)
Episode: 3765 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 3766 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1308, grad_fn=<SubBackward0>)
Episode: 3767 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8565, grad_fn=<SubBackward0>)
Episode: 3768 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1279, grad_fn=<SubBackward0>)
Episode: 3769 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3770 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1304, grad_fn=<SubBackward0>)
Episode: 3771 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3772 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(0.9799, grad_fn=<SubBackward0>)
Episode: 3773 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3774 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1340, grad_fn=<SubBackward0>)
Episode: 3775 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1364, grad_fn=<SubBackward0>)
Episode: 3776 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 3777 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1291, grad_fn=<SubBackward0>)
Episode: 3778 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9434, grad_fn=<SubBackward0>)
Episode: 3779 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 3780 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1309, grad_fn=<SubBackward0>)
Episode: 3781 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1311, grad_fn=<SubBackward0>)
Episode: 3782 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1359, grad_fn=<SubBackward0>)
Episode: 3783 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 3784 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1325, grad_fn=<SubBackward0>)
Episode: 3785 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0073, grad_fn=<SubBackward0>)
Episode: 3786 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9521, grad_fn=<SubBackward0>)
Episode: 3787 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1329, grad_fn=<SubBackward0>)
Episode: 3788 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1326, grad_fn=<SubBackward0>)
Episode: 3789 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1346, grad_fn=<SubBackward0>)
Episode: 3790 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1303, grad_fn=<SubBackward0>)
Episode: 3791 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1345, grad_fn=<SubBackward0>)
Episode: 3792 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1320, grad_fn=<SubBackward0>)
Episode: 3793 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1260, grad_fn=<SubBackward0>)
Episode: 3794 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7297, grad_fn=<SubBackward0>)
Episode: 3795 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1319, grad_fn=<SubBackward0>)
Episode: 3796 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1318, grad_fn=<SubBackward0>)
Episode: 3797 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1272, grad_fn=<SubBackward0>)
Episode: 3798 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3799 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1298, grad_fn=<SubBackward0>)
Episode: 3800 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1302, grad_fn=<SubBackward0>)
Episode: 3801 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1301, grad_fn=<SubBackward0>)
Episode: 3802 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1227, grad_fn=<SubBackward0>)
Episode: 3803 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1307, grad_fn=<SubBackward0>)
Episode: 3804 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1351, grad_fn=<SubBackward0>)
Episode: 3805 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8600, grad_fn=<SubBackward0>)
Episode: 3806 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1295, grad_fn=<SubBackward0>)
Episode: 3807 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3808 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1270, grad_fn=<SubBackward0>)
Episode: 3809 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1294, grad_fn=<SubBackward0>)
Episode: 3810 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1296, grad_fn=<SubBackward0>)
Episode: 3811 | Train Reward: tensor([1.]) | Train Loss: tensor(0.9770, grad_fn=<SubBackward0>)
Episode: 3812 | Train Reward: tensor([1.]) | Train Loss: tensor(1.0297, grad_fn=<SubBackward0>)
Episode: 3813 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1317, grad_fn=<SubBackward0>)
Episode: 3814 | Train Reward: tensor([0., 0., 0., 1.]) | Train Loss: tensor(1.0714, grad_fn=<SubBackward0>)
Episode: 3815 | Train Reward: tensor([0., 1.]) | Train Loss: tensor(0.9251, grad_fn=<SubBackward0>)
Episode: 3816 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(0.8895, grad_fn=<SubBackward0>)
Episode: 3817 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1292, grad_fn=<SubBackward0>)
Episode: 3818 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1250, grad_fn=<SubBackward0>)
Episode: 3819 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.7956, grad_fn=<SubBackward0>)
Episode: 3820 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1274, grad_fn=<SubBackward0>)
Episode: 3821 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1313, grad_fn=<SubBackward0>)
Episode: 3822 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1215, grad_fn=<SubBackward0>)
Episode: 3823 | Train Reward: tensor([0., 0., 0., 0., 0., 1.]) | Train Loss: tensor(0.9152, grad_fn=<SubBackward0>)
Episode: 3824 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1271, grad_fn=<SubBackward0>)
Episode: 3825 | Train Reward: tensor([1.]) | Train Loss: tensor(1.1560, grad_fn=<SubBackward0>)
Episode: 3826 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1245, grad_fn=<SubBackward0>)
Episode: 3827 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1265, grad_fn=<SubBackward0>)
Episode: 3828 | Train Reward: tensor([0., 0., 1.]) | Train Loss: tensor(1.2036, grad_fn=<SubBackward0>)
Episode: 3829 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1258, grad_fn=<SubBackward0>)
Episode: 3830 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1235, grad_fn=<SubBackward0>)
Episode: 3831 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1202, grad_fn=<SubBackward0>)
Episode: 3832 | Train Reward: tensor([0., 0., 0., 0., 1.]) | Train Loss: tensor(1.0912, grad_fn=<SubBackward0>)
Episode: 3833 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1276, grad_fn=<SubBackward0>)
Episode: 3834 | Train Reward: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) | Train Loss: tensor(-0.1264, grad_fn=<SubBackward0>)
